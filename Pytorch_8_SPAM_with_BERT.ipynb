{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "85c15f62",
      "metadata": {
        "id": "85c15f62",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a3a4d06-5965-4154-bc8b-b75b41b550d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.10/dist-packages (1.2.1)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.23.5)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (23.2)\n",
            "Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.1.0+cu118)\n",
            "Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (0.10.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (67.7.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.1->torchmetrics) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.1->torchmetrics) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torchmetrics"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from tabulate import tabulate\n",
        "from tqdm import trange\n",
        "import random\n",
        "from torchmetrics.classification import Recall, Accuracy, AUROC, Precision"
      ],
      "metadata": {
        "id": "3N_ngQVUPqCD"
      },
      "id": "3N_ngQVUPqCD",
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "6258c581",
      "metadata": {
        "id": "6258c581"
      },
      "outputs": [],
      "source": [
        "FILL_IN = \"FILL_IN\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "3678adfa",
      "metadata": {
        "id": "3678adfa",
        "outputId": "a5b699c7-4e05-48cd-d399-209f0de3c0a7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-12-07 01:33:07--  https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified\n",
            "Saving to: ‘smsspamcollection.zip.1’\n",
            "\n",
            "smsspamcollection.z     [ <=>                ] 198.65K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2023-12-07 01:33:07 (1.37 MB/s) - ‘smsspamcollection.zip.1’ saved [203415]\n",
            "\n",
            "Archive:  smsspamcollection.zip\n",
            "  inflating: SMSSpamCollection       \n",
            "  inflating: readme                  \n"
          ]
        }
      ],
      "source": [
        "# You can do this or just add the zip file I give you\n",
        "!wget 'https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip'\n",
        "!unzip -o smsspamcollection.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "dd3d5e84",
      "metadata": {
        "id": "dd3d5e84",
        "outputId": "e8fe0d05-694d-4cfe-c81f-29257577c257",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ham\tGo until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\n",
            "ham\tOk lar... Joking wif u oni...\n",
            "spam\tFree entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\n",
            "ham\tU dun say so early hor... U c already then say...\n",
            "ham\tNah I don't think he goes to usf, he lives around here though\n",
            "spam\tFreeMsg Hey there darling it's been 3 week's now and no word back! I'd like some fun you up for it still? Tb ok! XxX std chgs to send, £1.50 to rcv\n",
            "ham\tEven my brother is not like to speak with me. They treat me like aids patent.\n",
            "ham\tAs per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your callertune for all Callers. Press *9 to copy your friends Callertune\n",
            "spam\tWINNER!! As a valued network customer you have been selected to receivea £900 prize reward! To claim call 09061701461. Claim code KL341. Valid 12 hours only.\n",
            "spam\tHad your mobile 11 months or more? U R entitled to Update to the latest colour mobiles with camera for Free! Call The Mobile Update Co FREE on 08002986030\n"
          ]
        }
      ],
      "source": [
        "!head -10 SMSSpamCollection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "98a736a1",
      "metadata": {
        "id": "98a736a1",
        "outputId": "f44cf5e1-766a-411e-ba0d-bf01a16a10b3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   label                                               text\n",
            "0      0  Go until jurong point, crazy.. Available only ...\n",
            "1      0                      Ok lar... Joking wif u oni...\n",
            "2      1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
            "3      0  U dun say so early hor... U c already then say...\n",
            "4      0  Nah I don't think he goes to usf, he lives aro...\n",
            "5      1  FreeMsg Hey there darling it's been 3 week's n...\n",
            "6      0  Even my brother is not like to speak with me. ...\n",
            "7      0  As per your request 'Melle Melle (Oru Minnamin...\n",
            "8      1  WINNER!! As a valued network customer you have...\n",
            "9      1  Had your mobile 11 months or more? U R entitle...\n"
          ]
        }
      ],
      "source": [
        "# Loop through the data and make a dataframe with two columns\n",
        "# label is 0/1 if Not Spam / Spam\n",
        "df = pd.read_csv(\"SMSSpamCollection\", sep='\\t', header=None, names=['label', 'text'])\n",
        "\n",
        "# Map 'ham' to 0 and 'spam' to 1 in the 'label' column\n",
        "df['label'] = df['label'].map({'ham': 0, 'spam': 1})\n",
        "\n",
        "# Display the first 10 rows of the DataFrame\n",
        "print(df.head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "79a5cb02",
      "metadata": {
        "id": "79a5cb02"
      },
      "outputs": [],
      "source": [
        "text = df.text.values\n",
        "labels = df.label.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "dce193a4",
      "metadata": {
        "id": "dce193a4"
      },
      "outputs": [],
      "source": [
        "# Get the tokenizer for BERT, using 'bert-based-uncased'\n",
        "# Set do_lower_case = True\n",
        "\n",
        "# Specify the BERT model name\n",
        "model_name = 'bert-base-uncased'\n",
        "\n",
        "# Create the tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name, do_lower_case=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "c00a3e7e",
      "metadata": {
        "id": "c00a3e7e",
        "outputId": "93bd1297-9560-41a3-ebd8-ae3ff5f0a6e1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "╒══════════╤═════════════╕\n",
            "│ Tokens   │   Token IDs │\n",
            "╞══════════╪═════════════╡\n",
            "│ mm       │        3461 │\n",
            "├──────────┼─────────────┤\n",
            "│ umm      │       26114 │\n",
            "├──────────┼─────────────┤\n",
            "│ ##a      │        2050 │\n",
            "├──────────┼─────────────┤\n",
            "│ ask      │        3198 │\n",
            "├──────────┼─────────────┤\n",
            "│ va       │       12436 │\n",
            "├──────────┼─────────────┤\n",
            "│ ##va     │        3567 │\n",
            "├──────────┼─────────────┤\n",
            "│ also     │        2036 │\n",
            "├──────────┼─────────────┤\n",
            "│ to       │        2000 │\n",
            "├──────────┼─────────────┤\n",
            "│ come     │        2272 │\n",
            "├──────────┼─────────────┤\n",
            "│ tell     │        2425 │\n",
            "├──────────┼─────────────┤\n",
            "│ him      │        2032 │\n",
            "├──────────┼─────────────┤\n",
            "│ can      │        2064 │\n",
            "├──────────┼─────────────┤\n",
            "│ play     │        2377 │\n",
            "├──────────┼─────────────┤\n",
            "│ later    │        2101 │\n",
            "├──────────┼─────────────┤\n",
            "│ together │        2362 │\n",
            "╘══════════╧═════════════╛\n"
          ]
        }
      ],
      "source": [
        "def print_rand_sentence():\n",
        "    '''Displays the tokens and respective IDs of a random text sample'''\n",
        "    index = random.randint(0, len(text)-1)\n",
        "\n",
        "    # Tokenize a random sentence's text and display the token / token id\n",
        "    # You might need to use \"tabulate\" on a certain numpy array\n",
        "    FILL_IN\n",
        "    random_sentence = text[index]\n",
        "    # Tokenize the random sentence\n",
        "    tokens = tokenizer.tokenize(random_sentence)\n",
        "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "    # Display the token and token ID information using tabulate\n",
        "    table = zip(tokens, token_ids)\n",
        "    headers = ['Tokens', 'Token IDs']\n",
        "    print(tabulate(table, headers=headers, tablefmt=\"fancy_grid\"))\n",
        "\n",
        "print_rand_sentence()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "e85d0895",
      "metadata": {
        "id": "e85d0895",
        "outputId": "7fe4c14e-15ce-4ce9-efa3-10d8a3abfae7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "token_id = []\n",
        "attention_masks = []\n",
        "\n",
        "def preprocessing(input_text, tokenizer):\n",
        "  '''\n",
        "  Returns <class transformers.tokenization_utils_base.BatchEncoding> with the following fields:\n",
        "    - input_ids: list of token ids\n",
        "    - token_type_ids: list of token type ids\n",
        "    - attention_mask: list of indices (0,1) specifying which tokens should considered by the model (return_attention_mask = True).\n",
        "  '''\n",
        "    # Use the tokenizer to preprocess text\n",
        "    # add_special_tokens = True, let the max_length = 32, pad_to_max_length = True, return_tensors = 'pt'\n",
        "    # Look up tokenizer.encode_plus\n",
        "  encoding_dict = tokenizer.encode_plus(\n",
        "        input_text,\n",
        "        add_special_tokens=True,\n",
        "        max_length=32,\n",
        "        pad_to_max_length=True,\n",
        "        return_attention_mask=True,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "  return encoding_dict\n",
        "\n",
        "# For each sentence, loop and preprocess it\n",
        "# Put the input ids in the token_id array\n",
        "# Put the attention_masks in the attention_masks array\n",
        "\n",
        "for sample in text:\n",
        "    encoding_dict = preprocessing(sample, tokenizer)\n",
        "    token_id.append(encoding_dict['input_ids'])\n",
        "    attention_masks.append(encoding_dict['attention_mask'])\n",
        "\n",
        "# Concatenate all the elements of token_id into a tensor\n",
        "token_id = torch.cat(token_id, dim=0)\n",
        "# Concatenate all the elements of attention_masks\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "# Make a tensor out of the labels\n",
        "labels = torch.tensor(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "222e06a7",
      "metadata": {
        "id": "222e06a7",
        "outputId": "7b53c237-25ce-4c52-dead-7230966833e8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "╒══════════╤═════════════╤═══════════════════╕\n",
            "│ Tokens   │   Token IDs │   Attention Masks │\n",
            "╞══════════╪═════════════╪═══════════════════╡\n",
            "│ [CLS]    │         101 │                 1 │\n",
            "├──────────┼─────────────┼───────────────────┤\n",
            "│ i        │        1045 │                 1 │\n",
            "├──────────┼─────────────┼───────────────────┤\n",
            "│ want     │        2215 │                 1 │\n",
            "├──────────┼─────────────┼───────────────────┤\n",
            "│ to       │        2000 │                 1 │\n",
            "├──────────┼─────────────┼───────────────────┤\n",
            "│ be       │        2022 │                 1 │\n",
            "├──────────┼─────────────┼───────────────────┤\n",
            "│ there    │        2045 │                 1 │\n",
            "├──────────┼─────────────┼───────────────────┤\n",
            "│ so       │        2061 │                 1 │\n",
            "├──────────┼─────────────┼───────────────────┤\n",
            "│ i        │        1045 │                 1 │\n",
            "├──────────┼─────────────┼───────────────────┤\n",
            "│ can      │        2064 │                 1 │\n",
            "├──────────┼─────────────┼───────────────────┤\n",
            "│ kiss     │        3610 │                 1 │\n",
            "├──────────┼─────────────┼───────────────────┤\n",
            "│ you      │        2017 │                 1 │\n",
            "├──────────┼─────────────┼───────────────────┤\n",
            "│ and      │        1998 │                 1 │\n",
            "├──────────┼─────────────┼───────────────────┤\n",
            "│ feel     │        2514 │                 1 │\n",
            "├──────────┼─────────────┼───────────────────┤\n",
            "│ you      │        2017 │                 1 │\n",
            "├──────────┼─────────────┼───────────────────┤\n",
            "│ next     │        2279 │                 1 │\n",
            "├──────────┼─────────────┼───────────────────┤\n",
            "│ to       │        2000 │                 1 │\n",
            "├──────────┼─────────────┼───────────────────┤\n",
            "│ me       │        2033 │                 1 │\n",
            "├──────────┼─────────────┼───────────────────┤\n",
            "│ [SEP]    │         102 │                 1 │\n",
            "├──────────┼─────────────┼───────────────────┤\n",
            "│ [PAD]    │           0 │                 0 │\n",
            "├──────────┼─────────────┼───────────────────┤\n",
            "│ [PAD]    │           0 │                 0 │\n",
            "├──────────┼─────────────┼───────────────────┤\n",
            "│ [PAD]    │           0 │                 0 │\n",
            "├──────────┼─────────────┼───────────────────┤\n",
            "│ [PAD]    │           0 │                 0 │\n",
            "├──────────┼─────────────┼───────────────────┤\n",
            "│ [PAD]    │           0 │                 0 │\n",
            "├──────────┼─────────────┼───────────────────┤\n",
            "│ [PAD]    │           0 │                 0 │\n",
            "├──────────┼─────────────┼───────────────────┤\n",
            "│ [PAD]    │           0 │                 0 │\n",
            "├──────────┼─────────────┼───────────────────┤\n",
            "│ [PAD]    │           0 │                 0 │\n",
            "├──────────┼─────────────┼───────────────────┤\n",
            "│ [PAD]    │           0 │                 0 │\n",
            "├──────────┼─────────────┼───────────────────┤\n",
            "│ [PAD]    │           0 │                 0 │\n",
            "├──────────┼─────────────┼───────────────────┤\n",
            "│ [PAD]    │           0 │                 0 │\n",
            "├──────────┼─────────────┼───────────────────┤\n",
            "│ [PAD]    │           0 │                 0 │\n",
            "├──────────┼─────────────┼───────────────────┤\n",
            "│ [PAD]    │           0 │                 0 │\n",
            "├──────────┼─────────────┼───────────────────┤\n",
            "│ [PAD]    │           0 │                 0 │\n",
            "╘══════════╧═════════════╧═══════════════════╛\n"
          ]
        }
      ],
      "source": [
        "def print_rand_sentence_encoding():\n",
        "    '''Displays tokens, token IDs and attention mask of a random text sample'''\n",
        "    index = random.randint(0, len(text) - 1)\n",
        "    # This should take a random row of token_id and display the tokens, token ids, and attention masks 1/0 values\n",
        "    # You might need to use \"tabulate\" on a certain numpy array\n",
        "    random_token_id = token_id[index]\n",
        "    random_attention_mask = attention_masks[index]\n",
        "    # Convert token IDs to tokens using the tokenizer's decode method\n",
        "    tokens = tokenizer.decode(random_token_id.tolist(), skip_special_tokens=False)\n",
        "\n",
        "    # Create a table using tabulate\n",
        "    table_data = list(zip(tokens.split(), random_token_id.tolist(), random_attention_mask.tolist()))\n",
        "    headers = [\"Tokens\", \"Token IDs\", \"Attention Masks\"]\n",
        "    table = tabulate(table_data, headers, tablefmt=\"fancy_grid\")\n",
        "\n",
        "    # Display the table\n",
        "    print(table)\n",
        "\n",
        "\n",
        "print_rand_sentence_encoding()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "e1c2c10b",
      "metadata": {
        "id": "e1c2c10b"
      },
      "outputs": [],
      "source": [
        "val_ratio = 0.2\n",
        "# Pick a recommended batch size from https://arxiv.org/pdf/1810.04805.pdf\n",
        "batch_size = 32\n",
        "\n",
        "# Indices of the train and validation splits stratified by labels\n",
        "train_idx, val_idx = train_test_split(\n",
        "    range(len(text)),\n",
        "    test_size=val_ratio,\n",
        "    stratify=labels,\n",
        "    random_state=0\n",
        ")\n",
        "\n",
        "# Train and validation sets\n",
        "train_set = TensorDataset(token_id[train_idx], attention_masks[train_idx], labels[train_idx])\n",
        "\n",
        "val_set = TensorDataset(token_id[val_idx], attention_masks[val_idx], labels[val_idx])\n",
        "\n",
        "# Prepare DataLoader\n",
        "train_dataloader = DataLoader(\n",
        "            train_set,\n",
        "            sampler = RandomSampler(train_set),\n",
        "            batch_size = batch_size\n",
        "        )\n",
        "\n",
        "validation_dataloader = DataLoader(\n",
        "            val_set,\n",
        "            sampler = SequentialSampler(val_set),\n",
        "            batch_size = batch_size\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "155cdefa",
      "metadata": {
        "id": "155cdefa"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "40bcf8c4",
      "metadata": {
        "id": "40bcf8c4"
      },
      "source": [
        "### Load specific versions of the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "f85fc88e",
      "metadata": {
        "id": "f85fc88e",
        "outputId": "268781c9-65c0-455a-8d4b-625651d495e5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Load the BertForSequenceClassification model\n",
        "# Do not ouput the attentions and all hidden states\n",
        "from transformers import BertForSequenceClassification, AdamW\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\", num_labels=2, output_attentions=False, output_hidden_states=False)\n",
        "\n",
        "# See for the optimizer and some learning rates: https://arxiv.org/pdf/1810.04805.pdf\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "c5ea7837",
      "metadata": {
        "id": "c5ea7837"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "a36b60bf",
      "metadata": {
        "id": "a36b60bf"
      },
      "source": [
        "### Set the model to the right device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "071b1a84",
      "metadata": {
        "id": "071b1a84"
      },
      "outputs": [],
      "source": [
        "# device = torch.device('mps') if (\n",
        "#     torch.backends.mps.is_available() and torch.backends.mps.is_built()\n",
        "# ) else torch.device('cpu')\n",
        "\n",
        "# If on GPU, do as below\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "4811b9f5",
      "metadata": {
        "id": "4811b9f5"
      },
      "outputs": [],
      "source": [
        "_ = model.to(device)\n",
        "\n",
        "# Recommended number of epochs: See: https://arxiv.org/pdf/1810.04805.pdf\n",
        "epochs = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "f9f4e75e",
      "metadata": {
        "id": "f9f4e75e",
        "outputId": "d687f17d-1146-4779-d91f-d9c591f84362",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bert.embeddings.word_embeddings.weight torch.Size([30522, 768])\n",
            "bert.embeddings.position_embeddings.weight torch.Size([512, 768])\n",
            "bert.embeddings.token_type_embeddings.weight torch.Size([2, 768])\n",
            "bert.embeddings.LayerNorm.weight torch.Size([768])\n",
            "bert.embeddings.LayerNorm.bias torch.Size([768])\n",
            "bert.encoder.layer.0.attention.self.query.weight torch.Size([768, 768])\n",
            "bert.encoder.layer.0.attention.self.query.bias torch.Size([768])\n",
            "bert.encoder.layer.0.attention.self.key.weight torch.Size([768, 768])\n",
            "bert.encoder.layer.0.attention.self.key.bias torch.Size([768])\n",
            "bert.encoder.layer.0.attention.self.value.weight torch.Size([768, 768])\n",
            "bert.encoder.layer.0.attention.self.value.bias torch.Size([768])\n",
            "bert.encoder.layer.0.attention.output.dense.weight torch.Size([768, 768])\n",
            "bert.encoder.layer.0.attention.output.dense.bias torch.Size([768])\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.weight torch.Size([768])\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.bias torch.Size([768])\n",
            "bert.encoder.layer.0.intermediate.dense.weight torch.Size([3072, 768])\n",
            "bert.encoder.layer.0.intermediate.dense.bias torch.Size([3072])\n",
            "bert.encoder.layer.0.output.dense.weight torch.Size([768, 3072])\n",
            "bert.encoder.layer.0.output.dense.bias torch.Size([768])\n",
            "bert.encoder.layer.0.output.LayerNorm.weight torch.Size([768])\n",
            "bert.encoder.layer.0.output.LayerNorm.bias torch.Size([768])\n",
            "bert.encoder.layer.1.attention.self.query.weight torch.Size([768, 768])\n",
            "bert.encoder.layer.1.attention.self.query.bias torch.Size([768])\n",
            "bert.encoder.layer.1.attention.self.key.weight torch.Size([768, 768])\n",
            "bert.encoder.layer.1.attention.self.key.bias torch.Size([768])\n",
            "bert.encoder.layer.1.attention.self.value.weight torch.Size([768, 768])\n",
            "bert.encoder.layer.1.attention.self.value.bias torch.Size([768])\n",
            "bert.encoder.layer.1.attention.output.dense.weight torch.Size([768, 768])\n",
            "bert.encoder.layer.1.attention.output.dense.bias torch.Size([768])\n",
            "bert.encoder.layer.1.attention.output.LayerNorm.weight torch.Size([768])\n",
            "bert.encoder.layer.1.attention.output.LayerNorm.bias torch.Size([768])\n",
            "bert.encoder.layer.1.intermediate.dense.weight torch.Size([3072, 768])\n",
            "bert.encoder.layer.1.intermediate.dense.bias torch.Size([3072])\n",
            "bert.encoder.layer.1.output.dense.weight torch.Size([768, 3072])\n",
            "bert.encoder.layer.1.output.dense.bias torch.Size([768])\n",
            "bert.encoder.layer.1.output.LayerNorm.weight torch.Size([768])\n",
            "bert.encoder.layer.1.output.LayerNorm.bias torch.Size([768])\n",
            "bert.encoder.layer.2.attention.self.query.weight torch.Size([768, 768])\n",
            "bert.encoder.layer.2.attention.self.query.bias torch.Size([768])\n",
            "bert.encoder.layer.2.attention.self.key.weight torch.Size([768, 768])\n",
            "bert.encoder.layer.2.attention.self.key.bias torch.Size([768])\n",
            "bert.encoder.layer.2.attention.self.value.weight torch.Size([768, 768])\n",
            "bert.encoder.layer.2.attention.self.value.bias torch.Size([768])\n",
            "bert.encoder.layer.2.attention.output.dense.weight torch.Size([768, 768])\n",
            "bert.encoder.layer.2.attention.output.dense.bias torch.Size([768])\n",
            "bert.encoder.layer.2.attention.output.LayerNorm.weight torch.Size([768])\n",
            "bert.encoder.layer.2.attention.output.LayerNorm.bias torch.Size([768])\n",
            "bert.encoder.layer.2.intermediate.dense.weight torch.Size([3072, 768])\n",
            "bert.encoder.layer.2.intermediate.dense.bias torch.Size([3072])\n",
            "bert.encoder.layer.2.output.dense.weight torch.Size([768, 3072])\n",
            "bert.encoder.layer.2.output.dense.bias torch.Size([768])\n",
            "bert.encoder.layer.2.output.LayerNorm.weight torch.Size([768])\n",
            "bert.encoder.layer.2.output.LayerNorm.bias torch.Size([768])\n",
            "bert.encoder.layer.3.attention.self.query.weight torch.Size([768, 768])\n",
            "bert.encoder.layer.3.attention.self.query.bias torch.Size([768])\n",
            "bert.encoder.layer.3.attention.self.key.weight torch.Size([768, 768])\n",
            "bert.encoder.layer.3.attention.self.key.bias torch.Size([768])\n",
            "bert.encoder.layer.3.attention.self.value.weight torch.Size([768, 768])\n",
            "bert.encoder.layer.3.attention.self.value.bias torch.Size([768])\n",
            "bert.encoder.layer.3.attention.output.dense.weight torch.Size([768, 768])\n",
            "bert.encoder.layer.3.attention.output.dense.bias torch.Size([768])\n",
            "bert.encoder.layer.3.attention.output.LayerNorm.weight torch.Size([768])\n",
            "bert.encoder.layer.3.attention.output.LayerNorm.bias torch.Size([768])\n",
            "bert.encoder.layer.3.intermediate.dense.weight torch.Size([3072, 768])\n",
            "bert.encoder.layer.3.intermediate.dense.bias torch.Size([3072])\n",
            "bert.encoder.layer.3.output.dense.weight torch.Size([768, 3072])\n",
            "bert.encoder.layer.3.output.dense.bias torch.Size([768])\n",
            "bert.encoder.layer.3.output.LayerNorm.weight torch.Size([768])\n",
            "bert.encoder.layer.3.output.LayerNorm.bias torch.Size([768])\n",
            "bert.encoder.layer.4.attention.self.query.weight torch.Size([768, 768])\n",
            "bert.encoder.layer.4.attention.self.query.bias torch.Size([768])\n",
            "bert.encoder.layer.4.attention.self.key.weight torch.Size([768, 768])\n",
            "bert.encoder.layer.4.attention.self.key.bias torch.Size([768])\n",
            "bert.encoder.layer.4.attention.self.value.weight torch.Size([768, 768])\n",
            "bert.encoder.layer.4.attention.self.value.bias torch.Size([768])\n",
            "bert.encoder.layer.4.attention.output.dense.weight torch.Size([768, 768])\n",
            "bert.encoder.layer.4.attention.output.dense.bias torch.Size([768])\n",
            "bert.encoder.layer.4.attention.output.LayerNorm.weight torch.Size([768])\n",
            "bert.encoder.layer.4.attention.output.LayerNorm.bias torch.Size([768])\n",
            "bert.encoder.layer.4.intermediate.dense.weight torch.Size([3072, 768])\n",
            "bert.encoder.layer.4.intermediate.dense.bias torch.Size([3072])\n",
            "bert.encoder.layer.4.output.dense.weight torch.Size([768, 3072])\n",
            "bert.encoder.layer.4.output.dense.bias torch.Size([768])\n",
            "bert.encoder.layer.4.output.LayerNorm.weight torch.Size([768])\n",
            "bert.encoder.layer.4.output.LayerNorm.bias torch.Size([768])\n",
            "bert.encoder.layer.5.attention.self.query.weight torch.Size([768, 768])\n",
            "bert.encoder.layer.5.attention.self.query.bias torch.Size([768])\n",
            "bert.encoder.layer.5.attention.self.key.weight torch.Size([768, 768])\n",
            "bert.encoder.layer.5.attention.self.key.bias torch.Size([768])\n",
            "bert.encoder.layer.5.attention.self.value.weight torch.Size([768, 768])\n",
            "bert.encoder.layer.5.attention.self.value.bias torch.Size([768])\n",
            "bert.encoder.layer.5.attention.output.dense.weight torch.Size([768, 768])\n",
            "bert.encoder.layer.5.attention.output.dense.bias torch.Size([768])\n",
            "bert.encoder.layer.5.attention.output.LayerNorm.weight torch.Size([768])\n",
            "bert.encoder.layer.5.attention.output.LayerNorm.bias torch.Size([768])\n",
            "bert.encoder.layer.5.intermediate.dense.weight torch.Size([3072, 768])\n",
            "bert.encoder.layer.5.intermediate.dense.bias torch.Size([3072])\n",
            "bert.encoder.layer.5.output.dense.weight torch.Size([768, 3072])\n",
            "bert.encoder.layer.5.output.dense.bias torch.Size([768])\n",
            "bert.encoder.layer.5.output.LayerNorm.weight torch.Size([768])\n",
            "bert.encoder.layer.5.output.LayerNorm.bias torch.Size([768])\n",
            "bert.encoder.layer.6.attention.self.query.weight torch.Size([768, 768])\n",
            "bert.encoder.layer.6.attention.self.query.bias torch.Size([768])\n",
            "bert.encoder.layer.6.attention.self.key.weight torch.Size([768, 768])\n",
            "bert.encoder.layer.6.attention.self.key.bias torch.Size([768])\n",
            "bert.encoder.layer.6.attention.self.value.weight torch.Size([768, 768])\n",
            "bert.encoder.layer.6.attention.self.value.bias torch.Size([768])\n",
            "bert.encoder.layer.6.attention.output.dense.weight torch.Size([768, 768])\n",
            "bert.encoder.layer.6.attention.output.dense.bias torch.Size([768])\n",
            "bert.encoder.layer.6.attention.output.LayerNorm.weight torch.Size([768])\n",
            "bert.encoder.layer.6.attention.output.LayerNorm.bias torch.Size([768])\n",
            "bert.encoder.layer.6.intermediate.dense.weight torch.Size([3072, 768])\n",
            "bert.encoder.layer.6.intermediate.dense.bias torch.Size([3072])\n",
            "bert.encoder.layer.6.output.dense.weight torch.Size([768, 3072])\n",
            "bert.encoder.layer.6.output.dense.bias torch.Size([768])\n",
            "bert.encoder.layer.6.output.LayerNorm.weight torch.Size([768])\n",
            "bert.encoder.layer.6.output.LayerNorm.bias torch.Size([768])\n",
            "bert.encoder.layer.7.attention.self.query.weight torch.Size([768, 768])\n",
            "bert.encoder.layer.7.attention.self.query.bias torch.Size([768])\n",
            "bert.encoder.layer.7.attention.self.key.weight torch.Size([768, 768])\n",
            "bert.encoder.layer.7.attention.self.key.bias torch.Size([768])\n",
            "bert.encoder.layer.7.attention.self.value.weight torch.Size([768, 768])\n",
            "bert.encoder.layer.7.attention.self.value.bias torch.Size([768])\n",
            "bert.encoder.layer.7.attention.output.dense.weight torch.Size([768, 768])\n",
            "bert.encoder.layer.7.attention.output.dense.bias torch.Size([768])\n",
            "bert.encoder.layer.7.attention.output.LayerNorm.weight torch.Size([768])\n",
            "bert.encoder.layer.7.attention.output.LayerNorm.bias torch.Size([768])\n",
            "bert.encoder.layer.7.intermediate.dense.weight torch.Size([3072, 768])\n",
            "bert.encoder.layer.7.intermediate.dense.bias torch.Size([3072])\n",
            "bert.encoder.layer.7.output.dense.weight torch.Size([768, 3072])\n",
            "bert.encoder.layer.7.output.dense.bias torch.Size([768])\n",
            "bert.encoder.layer.7.output.LayerNorm.weight torch.Size([768])\n",
            "bert.encoder.layer.7.output.LayerNorm.bias torch.Size([768])\n",
            "bert.encoder.layer.8.attention.self.query.weight torch.Size([768, 768])\n",
            "bert.encoder.layer.8.attention.self.query.bias torch.Size([768])\n",
            "bert.encoder.layer.8.attention.self.key.weight torch.Size([768, 768])\n",
            "bert.encoder.layer.8.attention.self.key.bias torch.Size([768])\n",
            "bert.encoder.layer.8.attention.self.value.weight torch.Size([768, 768])\n",
            "bert.encoder.layer.8.attention.self.value.bias torch.Size([768])\n",
            "bert.encoder.layer.8.attention.output.dense.weight torch.Size([768, 768])\n",
            "bert.encoder.layer.8.attention.output.dense.bias torch.Size([768])\n",
            "bert.encoder.layer.8.attention.output.LayerNorm.weight torch.Size([768])\n",
            "bert.encoder.layer.8.attention.output.LayerNorm.bias torch.Size([768])\n",
            "bert.encoder.layer.8.intermediate.dense.weight torch.Size([3072, 768])\n",
            "bert.encoder.layer.8.intermediate.dense.bias torch.Size([3072])\n",
            "bert.encoder.layer.8.output.dense.weight torch.Size([768, 3072])\n",
            "bert.encoder.layer.8.output.dense.bias torch.Size([768])\n",
            "bert.encoder.layer.8.output.LayerNorm.weight torch.Size([768])\n",
            "bert.encoder.layer.8.output.LayerNorm.bias torch.Size([768])\n",
            "bert.encoder.layer.9.attention.self.query.weight torch.Size([768, 768])\n",
            "bert.encoder.layer.9.attention.self.query.bias torch.Size([768])\n",
            "bert.encoder.layer.9.attention.self.key.weight torch.Size([768, 768])\n",
            "bert.encoder.layer.9.attention.self.key.bias torch.Size([768])\n",
            "bert.encoder.layer.9.attention.self.value.weight torch.Size([768, 768])\n",
            "bert.encoder.layer.9.attention.self.value.bias torch.Size([768])\n",
            "bert.encoder.layer.9.attention.output.dense.weight torch.Size([768, 768])\n",
            "bert.encoder.layer.9.attention.output.dense.bias torch.Size([768])\n",
            "bert.encoder.layer.9.attention.output.LayerNorm.weight torch.Size([768])\n",
            "bert.encoder.layer.9.attention.output.LayerNorm.bias torch.Size([768])\n",
            "bert.encoder.layer.9.intermediate.dense.weight torch.Size([3072, 768])\n",
            "bert.encoder.layer.9.intermediate.dense.bias torch.Size([3072])\n",
            "bert.encoder.layer.9.output.dense.weight torch.Size([768, 3072])\n",
            "bert.encoder.layer.9.output.dense.bias torch.Size([768])\n",
            "bert.encoder.layer.9.output.LayerNorm.weight torch.Size([768])\n",
            "bert.encoder.layer.9.output.LayerNorm.bias torch.Size([768])\n",
            "bert.encoder.layer.10.attention.self.query.weight torch.Size([768, 768])\n",
            "bert.encoder.layer.10.attention.self.query.bias torch.Size([768])\n",
            "bert.encoder.layer.10.attention.self.key.weight torch.Size([768, 768])\n",
            "bert.encoder.layer.10.attention.self.key.bias torch.Size([768])\n",
            "bert.encoder.layer.10.attention.self.value.weight torch.Size([768, 768])\n",
            "bert.encoder.layer.10.attention.self.value.bias torch.Size([768])\n",
            "bert.encoder.layer.10.attention.output.dense.weight torch.Size([768, 768])\n",
            "bert.encoder.layer.10.attention.output.dense.bias torch.Size([768])\n",
            "bert.encoder.layer.10.attention.output.LayerNorm.weight torch.Size([768])\n",
            "bert.encoder.layer.10.attention.output.LayerNorm.bias torch.Size([768])\n",
            "bert.encoder.layer.10.intermediate.dense.weight torch.Size([3072, 768])\n",
            "bert.encoder.layer.10.intermediate.dense.bias torch.Size([3072])\n",
            "bert.encoder.layer.10.output.dense.weight torch.Size([768, 3072])\n",
            "bert.encoder.layer.10.output.dense.bias torch.Size([768])\n",
            "bert.encoder.layer.10.output.LayerNorm.weight torch.Size([768])\n",
            "bert.encoder.layer.10.output.LayerNorm.bias torch.Size([768])\n",
            "bert.encoder.layer.11.attention.self.query.weight torch.Size([768, 768])\n",
            "bert.encoder.layer.11.attention.self.query.bias torch.Size([768])\n",
            "bert.encoder.layer.11.attention.self.key.weight torch.Size([768, 768])\n",
            "bert.encoder.layer.11.attention.self.key.bias torch.Size([768])\n",
            "bert.encoder.layer.11.attention.self.value.weight torch.Size([768, 768])\n",
            "bert.encoder.layer.11.attention.self.value.bias torch.Size([768])\n",
            "bert.encoder.layer.11.attention.output.dense.weight torch.Size([768, 768])\n",
            "bert.encoder.layer.11.attention.output.dense.bias torch.Size([768])\n",
            "bert.encoder.layer.11.attention.output.LayerNorm.weight torch.Size([768])\n",
            "bert.encoder.layer.11.attention.output.LayerNorm.bias torch.Size([768])\n",
            "bert.encoder.layer.11.intermediate.dense.weight torch.Size([3072, 768])\n",
            "bert.encoder.layer.11.intermediate.dense.bias torch.Size([3072])\n",
            "bert.encoder.layer.11.output.dense.weight torch.Size([768, 3072])\n",
            "bert.encoder.layer.11.output.dense.bias torch.Size([768])\n",
            "bert.encoder.layer.11.output.LayerNorm.weight torch.Size([768])\n",
            "bert.encoder.layer.11.output.LayerNorm.bias torch.Size([768])\n",
            "bert.pooler.dense.weight torch.Size([768, 768])\n",
            "bert.pooler.dense.bias torch.Size([768])\n",
            "classifier.weight torch.Size([2, 768])\n",
            "classifier.bias torch.Size([2])\n"
          ]
        }
      ],
      "source": [
        "# Print all the layers of this BERT model and the number of parameters per layer\n",
        "fine_tune_bert = True\n",
        "\n",
        "total_parameters = 0\n",
        "for name, param in model.named_parameters():\n",
        "    if fine_tune_bert:\n",
        "        print(name, param.shape)\n",
        "        total_parameters += param.numel()\n",
        "\n",
        "# Loop through all the parameters\n",
        "# if fine_tune_bert is off, just fine tune the classifer head; otherwise, all parameters\n",
        "# Also, print out all the parameter names and the number of elements for that parameter\n",
        "# Finally, accumulate the total number of parameters\n",
        "# You should get about 110 M\n",
        "\n",
        "assert(total_parameters == 109483778)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "dd1c248c",
      "metadata": {
        "id": "dd1c248c"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "765fc3a1",
      "metadata": {
        "id": "765fc3a1"
      },
      "source": [
        "### Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "1fd8d459",
      "metadata": {
        "id": "1fd8d459"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "bf0b712e",
      "metadata": {
        "id": "bf0b712e"
      },
      "outputs": [],
      "source": [
        "# Use torchmetrics to set up accuracy, recall, precision, and auroc\n",
        "# Put these all on cpu as AUROC does not work on mps due to some bug\n",
        "import torchmetrics\n",
        "\n",
        "accuracy = torchmetrics.Accuracy(task='binary').cpu()\n",
        "recall = torchmetrics.Recall(task='binary').cpu()\n",
        "precision = torchmetrics.Precision(task='binary').cpu()\n",
        "auroc = torchmetrics.AUROC(task='binary').cpu()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "084519f5",
      "metadata": {
        "id": "084519f5",
        "outputId": "d37516a9-edbb-40f9-de6f-9d1c50f2e529",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch:  50%|█████     | 1/2 [00:09<00:09,  9.53s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\t - Train loss: 0.0025\n",
            "\t - Validation Accuracy: 0.9911\n",
            "\t - Validation Precision: 0.9514\n",
            "\t - Validation Recall: 0.9181\n",
            "\t - Validation AUROC: 0.9438\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 100%|██████████| 2/2 [00:19<00:00,  9.52s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\t - Train loss: 0.0019\n",
            "\t - Validation Accuracy: 0.9884\n",
            "\t - Validation Precision: 0.9143\n",
            "\t - Validation Recall: 0.9312\n",
            "\t - Validation AUROC: 0.9479\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Main training / validation loop\n",
        "for _ in trange(epochs, desc = 'Epoch'):\n",
        "\n",
        "    # ========== Training ==========\n",
        "\n",
        "    # Set model to training mode\n",
        "    model.train()\n",
        "\n",
        "    # Tracking variables\n",
        "    tr_loss = 0\n",
        "    nb_tr_examples, nb_tr_steps = 0, 0\n",
        "\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # map each element of the batch to device\n",
        "        # Optimize over the batch\n",
        "        # Do the forward call, etc\n",
        "        # Remember to zero out the gradients as necessary\n",
        "\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        train_output = model(b_input_ids.to(device), attention_mask=b_input_mask.to(device), labels=b_labels)\n",
        "\n",
        "        # Backward pass\n",
        "        loss = train_output.loss\n",
        "        loss.backward()\n",
        "\n",
        "        # Anything else\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update tracking variables\n",
        "        tr_loss += loss.item()\n",
        "        nb_tr_examples += b_input_ids.size(0)\n",
        "        nb_tr_steps += 1\n",
        "\n",
        "    # ========== Validation ==========\n",
        "\n",
        "    # Set model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables\n",
        "    val_accuracy = []\n",
        "    val_precision = []\n",
        "    val_recall = []\n",
        "    val_auroc = []\n",
        "\n",
        "    for batch in validation_dataloader:\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        with torch.no_grad():\n",
        "            # Forward pass\n",
        "            eval_output = model(b_input_ids.to(device), token_type_ids=None, attention_mask=b_input_mask.to(device), labels = b_labels)\n",
        "\n",
        "        # Calculate validation metrics\n",
        "        labels = b_labels.to('cpu')\n",
        "        predicted_labels = torch.argmax(eval_output.logits, dim=1).to('cpu')\n",
        "\n",
        "        val_accuracy.append(accuracy(predicted_labels, labels))\n",
        "        val_recall.append(recall(predicted_labels, labels))\n",
        "        val_precision.append(precision(predicted_labels, labels))\n",
        "        val_auroc.append(auroc(predicted_labels, labels))\n",
        "\n",
        "    print('\\n\\t - Train loss: {:.4f}'.format(tr_loss / nb_tr_steps))\n",
        "    print('\\t - Validation Accuracy: {:.4f}'.format(sum(val_accuracy)/len(val_accuracy)))\n",
        "    print('\\t - Validation Precision: {:.4f}'.format(sum(val_precision)/len(val_precision)))\n",
        "    print('\\t - Validation Recall: {:.4f}'.format(sum(val_recall)/len(val_recall)))\n",
        "    print('\\t - Validation AUROC: {:.4f}\\n'.format(sum(val_auroc)/len(val_auroc)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c029bd94",
      "metadata": {
        "id": "c029bd94"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "13d64bc6",
      "metadata": {
        "id": "13d64bc6"
      },
      "source": [
        "### Test on a specific sentence, see the outcome"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "6dfcd9e7",
      "metadata": {
        "id": "6dfcd9e7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "925b7cda-e01c-421b-c698-5e15ca736a06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Sentence:  WINNER!! As a valued network customer you have been selected to receivea £900 prize reward! To claim call 09061701461. Claim code KL341. Valid 12 hours only.\n",
            "Predicted Class:  Spam\n"
          ]
        }
      ],
      "source": [
        "new_sentence = 'WINNER!! As a valued network customer you have been selected to receivea £900 prize reward! To claim call 09061701461. Claim code KL341. Valid 12 hours only.'\n",
        "\n",
        "# We need Token IDs and Attention Mask for inference on the new sentence\n",
        "test_ids = []\n",
        "test_attention_mask = []\n",
        "\n",
        "# Apply the tokenizer\n",
        "encoding = preprocessing(new_sentence, tokenizer)\n",
        "\n",
        "# Extract IDs and Attention Mask\n",
        "test_ids.append(encoding['input_ids'])\n",
        "test_attention_mask.append(encoding['attention_mask'])\n",
        "test_ids = torch.cat(test_ids, dim = 0)\n",
        "test_attention_mask = torch.cat(test_attention_mask, dim = 0)\n",
        "\n",
        "# Forward pass, calculate logit predictions\n",
        "with torch.no_grad():\n",
        "    output = model(test_ids.to(device), token_type_ids = None, attention_mask = test_attention_mask.to(device))\n",
        "\n",
        "prediction = 'Spam' if np.argmax(output.logits.cpu().numpy()).flatten().item() == 1 else 'Ham'\n",
        "\n",
        "print('Input Sentence: ', new_sentence)\n",
        "print('Predicted Class: ', prediction)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4453061",
      "metadata": {
        "id": "e4453061"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "a0c41741",
      "metadata": {
        "id": "a0c41741"
      },
      "source": [
        "### Questions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "875fab6d",
      "metadata": {
        "id": "875fab6d"
      },
      "source": [
        "Question 1: Run the above by fine tuning bert and the classfier head and by not doing this (using BERT as a feature encoder). What is the gap between this?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae772301",
      "metadata": {
        "id": "ae772301"
      },
      "source": [
        "Solution:\n",
        "\n",
        "If we don't tune the BERT part of the model, that is, we use BERT as a feature encoder and only tune the classification layer, the model performance drops.\n",
        "\n",
        "One the validation set:\n",
        "\n",
        "- If we tune BERT, Train loss: 0.0019. If not, Train loss: 0.6829.\n",
        "- If we tune BERT, Accuracy: 0.9884. If not, Accuracy: 0.8654.\n",
        "- If we tune BERT, Precision: 0.9143. If not, Precision: 0.\n",
        "- If we tune BERT, Recall: 0.9312. If not, Recall: 0.\n",
        "- If we tune BERT, AUROC: 0.9479. If not, AUROC: 0.4852.\n",
        "\n",
        "The AUROC drops almost half. If we do not tune BERT and only tune the classification layer, the model is not complex enough to capture the general pattern in the data. The precision and recall are nearly zero. So, the model may resort to always predict the mojority class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "id": "071f0f80",
      "metadata": {
        "id": "071f0f80",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9612e8ac-b958-4ded-a084-cc9b20835981"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "model2 = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\", num_labels=2, output_attentions=False, output_hidden_states=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Modify the last classification layer for binary classification (spam or not spam)\n",
        "num_classes = 2\n",
        "# model2.classifier = torch.nn.Sequential(\n",
        "#     torch.nn.Linear(model.config.hidden_size, num_classes),\n",
        "#     torch.nn.ReLU()\n",
        "# )\n",
        "# Freeze the BERT layers\n",
        "for param in model2.bert.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Set device\n",
        "model2.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eop4o36w_eQy",
        "outputId": "0dc04311-ffa3-4e7e-e1fe-bd76d6c10352"
      },
      "id": "Eop4o36w_eQy",
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Main training / validation loop\n",
        "for _ in trange(epochs, desc = 'Epoch'):\n",
        "\n",
        "    # ========== Training ==========\n",
        "\n",
        "    # Set model to training mode\n",
        "    model2.train()\n",
        "\n",
        "    # Tracking variables\n",
        "    tr_loss = 0\n",
        "    nb_tr_examples, nb_tr_steps = 0, 0\n",
        "\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # map each element of the batch to device\n",
        "        # Optimize over the batch\n",
        "        # Do the forward call, etc\n",
        "        # Remember to zero out the gradients as necessary\n",
        "\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        train_output = model2(b_input_ids.to(device), attention_mask=b_input_mask.to(device), labels=b_labels)\n",
        "\n",
        "        # Backward pass\n",
        "        loss = train_output.loss\n",
        "        loss.backward()\n",
        "\n",
        "        # Anything else\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update tracking variables\n",
        "        tr_loss += loss.item()\n",
        "        nb_tr_examples += b_input_ids.size(0)\n",
        "        nb_tr_steps += 1\n",
        "\n",
        "    # ========== Validation ==========\n",
        "\n",
        "    # Set model to evaluation mode\n",
        "    model2.eval()\n",
        "\n",
        "    # Tracking variables\n",
        "    val_accuracy = []\n",
        "    val_precision = []\n",
        "    val_recall = []\n",
        "    val_auroc = []\n",
        "\n",
        "    for batch in validation_dataloader:\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        with torch.no_grad():\n",
        "            # Forward pass\n",
        "            eval_output = model2(b_input_ids.to(device), token_type_ids=None, attention_mask=b_input_mask.to(device), labels = b_labels)\n",
        "\n",
        "        # Calculate validation metrics\n",
        "        labels = b_labels.to('cpu')\n",
        "        predicted_labels = torch.argmax(eval_output.logits, dim=1).to('cpu')\n",
        "\n",
        "        val_accuracy.append(accuracy(predicted_labels, labels))\n",
        "        val_recall.append(recall(predicted_labels, labels))\n",
        "        val_precision.append(precision(predicted_labels, labels))\n",
        "        val_auroc.append(auroc(predicted_labels, labels))\n",
        "\n",
        "    print('\\n\\t - Train loss: {:.4f}'.format(tr_loss / nb_tr_steps))\n",
        "    print('\\t - Validation Accuracy: {:.4f}'.format(sum(val_accuracy)/len(val_accuracy)))\n",
        "    print('\\t - Validation Precision: {:.4f}'.format(sum(val_precision)/len(val_precision)))\n",
        "    print('\\t - Validation Recall: {:.4f}'.format(sum(val_recall)/len(val_recall)))\n",
        "    print('\\t - Validation AUROC: {:.4f}\\n'.format(sum(val_auroc)/len(val_auroc)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N47xF4VM_eN5",
        "outputId": "c39431de-cbfb-4e5d-cba9-ec254b161e9d"
      },
      "id": "N47xF4VM_eN5",
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch:  50%|█████     | 1/2 [00:03<00:03,  3.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\t - Train loss: 0.4443\n",
            "\t - Validation Accuracy: 0.8654\n",
            "\t - Validation Precision: 0.0000\n",
            "\t - Validation Recall: 0.0000\n",
            "\t - Validation AUROC: 0.4852\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 100%|██████████| 2/2 [00:06<00:00,  3.30s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\t - Train loss: 0.4432\n",
            "\t - Validation Accuracy: 0.8654\n",
            "\t - Validation Precision: 0.0000\n",
            "\t - Validation Recall: 0.0000\n",
            "\t - Validation AUROC: 0.4852\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cU9dXnhj_eKZ"
      },
      "id": "cU9dXnhj_eKZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j22pNCzU_eBP"
      },
      "id": "j22pNCzU_eBP",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.15"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "gpuType": "V100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}