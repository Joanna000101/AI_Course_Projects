{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRfgHr2GyNTO"
      },
      "source": [
        "For this notebook, please insert where there is `_FILL_` either code or logic to make this work.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5XzVh-J0-fu"
      },
      "source": [
        "# MNIST CNN Digit Recognition Network\n",
        "\n",
        "For this problem, you will code a basic digit recognition network. The data are images which specify the digits 1 to 10 as (1, 28, 28) data - this data is black and white images. Each pixed of the image is an intensity between 0 and 255, and together the (1, 28, 28) pixel image can be visualized as a picture of a digit. The data is given to you as $\\{(x^{(i)}, y^{(i)})\\}_{i=1}^{N}$ where $y$ is the given label and x is the (1, 28, 28) data. This data will be gotten from `torchvision`, a repository of computer vision data and models.\n",
        "\n",
        "Highlevel, the model and notebook goes as follows:\n",
        "*   You first download the data and specify the batch size of B = 16. Each image will need to be turned from a (1, 28, 28) volume into a serious of other volumes either via convolutional layers or max pooling layers.\n",
        "*   You will pass the data through several layers to built a CNN classfier. Use the hints below to get the right dimensions and figure out what the layers should be. Be careful with the loss function. Add regularization (L1 and L2) manually.\n",
        "\n",
        "See the comments below and fill in the analysis where there is `_FILL_` specified. All asserts should pass and Test accuracy should be about 95%.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GfKuo0JFxJ7d",
        "outputId": "4e0f3ee0-c3cc-4d8d-a1e3-5c72101e03f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-1.2.0-py3-none-any.whl (805 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m805.2/805.2 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.23.5)\n",
            "Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.1.0+cu118)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
            "  Downloading lightning_utilities-0.9.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: packaging>=17.1 in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (23.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.1->torchmetrics) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.1->torchmetrics) (1.3.0)\n",
            "Installing collected packages: lightning-utilities, torchmetrics\n",
            "Successfully installed lightning-utilities-0.9.0 torchmetrics-1.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torchmetrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8UDIb4ldyj2C"
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "from torchvision import transforms\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch.nn as nn\n",
        "import torchmetrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "PdLoOr08AUY5"
      },
      "outputs": [],
      "source": [
        "SEED = 1\n",
        "torch.manual_seed(SEED)\n",
        "_FILL_ = '_FILL_'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jLD0oQmgxxlR",
        "outputId": "18c4b37f-dab0-4d6a-feaf-bc757bb86af2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 234576929.45it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./MNIST/raw/train-images-idx3-ubyte.gz to ./MNIST/raw\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 22016665.54it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./MNIST/raw/train-labels-idx1-ubyte.gz to ./MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 78121831.72it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./MNIST/raw/t10k-images-idx3-ubyte.gz to ./MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "100%|██████████| 4542/4542 [00:00<00:00, 7183457.30it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./MNIST/raw/t10k-labels-idx1-ubyte.gz to ./MNIST/raw\n",
            "\n"
          ]
        }
      ],
      "source": [
        "image_path = './'\n",
        "\n",
        "# Use ToTensor\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "mnist_train_dataset = torchvision.datasets.MNIST(\n",
        "    root=image_path,\n",
        "    train=True,\n",
        "    transform=transform,\n",
        "    download=True\n",
        "  )\n",
        "\n",
        "mnist_test_dataset = torchvision.datasets.MNIST(\n",
        "    root=image_path,\n",
        "    train=False,\n",
        "    transform=transform,\n",
        "    download=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "xGLuLaEXyzoD"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 16\n",
        "LR = 0.1\n",
        "L1_WEIGHT = 1e-10\n",
        "L2_WEIGHT = 1e-12\n",
        "EPOCHS = 20\n",
        "# Get the dataloader for train and test\n",
        "train_dl = DataLoader(mnist_train_dataset, BATCH_SIZE, shuffle=True)\n",
        "test_dl = DataLoader(mnist_test_dataset, BATCH_SIZE, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "PjLznvm8xqaT"
      },
      "outputs": [],
      "source": [
        "class CNNClassifier(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.cnn1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3)\n",
        "    self.cnn2 = nn.Conv2d(in_channels=32, out_channels=16, kernel_size=3)\n",
        "    self.cnn3 = nn.Conv2d(in_channels=16, out_channels=1, kernel_size=1)\n",
        "    self.linear = nn.Linear(in_features=25, out_features=10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    assert(x.shape == (BATCH_SIZE, 1, 28, 28))\n",
        "\n",
        "    # Pass through cnn layer 1\n",
        "    # (28, 28, 1) -> (26, 26, 32)\n",
        "    x = self.cnn1(x)\n",
        "    assert(x.shape == (BATCH_SIZE, 32, 26, 26))\n",
        "\n",
        "    # Pass through max pooling to give the result shape below\n",
        "    # (26, 26, 32) -> (13, 13, 32)\n",
        "    x = nn.functional.max_pool2d(x, kernel_size=2, stride=2)\n",
        "    assert(x.shape == (BATCH_SIZE, 32, 13, 13))\n",
        "\n",
        "    # Apply ReLU\n",
        "    x = nn.functional.relu(x)\n",
        "\n",
        "    # Pass through cnn layer 2 to give the result below\n",
        "    # (13, 13, 32) -> (11, 11, 16)\n",
        "    x = self.cnn2(x)\n",
        "    assert(x.shape == (BATCH_SIZE, 16, 11, 11))\n",
        "\n",
        "    # Pass through max pooling pool to give the result below\n",
        "    # (11, 11, 16) -> (5, 5, 16)\n",
        "    x = nn.functional.max_pool2d(x, kernel_size=2, stride=2)\n",
        "    assert(x.shape == (BATCH_SIZE, 16, 5, 5))\n",
        "\n",
        "    # Apply ReLU\n",
        "    x = nn.functional.relu(x)\n",
        "\n",
        "    # Pass through cnn layer 3 to give the result below\n",
        "    # (5, 5, 16) -> (5, 5, 1)\n",
        "    x = self.cnn3(x)\n",
        "    assert(x.shape == (BATCH_SIZE, 1, 5, 5))\n",
        "\n",
        "    # Apply ReLU\n",
        "    x = nn.functional.relu(x)\n",
        "\n",
        "    # Flatten to get the result below\n",
        "    # (5, 5, 1) - > (25, )\n",
        "    x = x.view(x.size(0), -1)\n",
        "    assert(x.shape == (BATCH_SIZE, 25))\n",
        "\n",
        "    # Pass through linear layer to get the result below\n",
        "    # (25, ) -> (16, ) #?? should be 10?\n",
        "    x = self.linear(x)\n",
        "    assert(x.shape == (BATCH_SIZE, 10))\n",
        "\n",
        "    # Return the logits\n",
        "    return x\n",
        "\n",
        "model = CNNClassifier()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zu4Z0aptxqcu",
        "outputId": "7ca987b5-2dc1-4f76-d5f3-99b3e1ff6eda"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Metrics Epoch 0 Loss 0.4114 Accuracy 0.8718\n",
            "Test Metrics Epoch 0 Loss 0.2177 Accuracy 0.9320\n",
            "Train Metrics Epoch 1 Loss 0.2289 Accuracy 0.9299\n",
            "Test Metrics Epoch 1 Loss 0.1655 Accuracy 0.9489\n",
            "Train Metrics Epoch 2 Loss 0.1926 Accuracy 0.9418\n",
            "Test Metrics Epoch 2 Loss 0.1787 Accuracy 0.9473\n",
            "Train Metrics Epoch 3 Loss 0.1765 Accuracy 0.9464\n",
            "Test Metrics Epoch 3 Loss 0.1465 Accuracy 0.9533\n",
            "Train Metrics Epoch 4 Loss 0.1664 Accuracy 0.9494\n",
            "Test Metrics Epoch 4 Loss 0.1904 Accuracy 0.9409\n",
            "Train Metrics Epoch 5 Loss 0.1550 Accuracy 0.9530\n",
            "Test Metrics Epoch 5 Loss 0.1330 Accuracy 0.9578\n",
            "Train Metrics Epoch 6 Loss 0.1507 Accuracy 0.9539\n",
            "Test Metrics Epoch 6 Loss 0.1424 Accuracy 0.9550\n",
            "Train Metrics Epoch 7 Loss 0.1457 Accuracy 0.9554\n",
            "Test Metrics Epoch 7 Loss 0.1236 Accuracy 0.9610\n",
            "Train Metrics Epoch 8 Loss 0.1396 Accuracy 0.9574\n",
            "Test Metrics Epoch 8 Loss 0.1268 Accuracy 0.9589\n",
            "Train Metrics Epoch 9 Loss 0.1362 Accuracy 0.9584\n",
            "Test Metrics Epoch 9 Loss 0.1287 Accuracy 0.9590\n",
            "Train Metrics Epoch 10 Loss 0.1336 Accuracy 0.9586\n",
            "Test Metrics Epoch 10 Loss 0.1334 Accuracy 0.9573\n",
            "Train Metrics Epoch 11 Loss 0.1298 Accuracy 0.9605\n",
            "Test Metrics Epoch 11 Loss 0.1364 Accuracy 0.9581\n",
            "Train Metrics Epoch 12 Loss 0.1265 Accuracy 0.9618\n",
            "Test Metrics Epoch 12 Loss 0.1297 Accuracy 0.9596\n",
            "Train Metrics Epoch 13 Loss 0.1269 Accuracy 0.9605\n",
            "Test Metrics Epoch 13 Loss 0.1451 Accuracy 0.9538\n",
            "Train Metrics Epoch 14 Loss 0.1228 Accuracy 0.9621\n",
            "Test Metrics Epoch 14 Loss 0.1297 Accuracy 0.9600\n",
            "Train Metrics Epoch 15 Loss 0.1211 Accuracy 0.9620\n",
            "Test Metrics Epoch 15 Loss 0.1515 Accuracy 0.9528\n",
            "Train Metrics Epoch 16 Loss 0.1209 Accuracy 0.9617\n",
            "Test Metrics Epoch 16 Loss 0.1257 Accuracy 0.9602\n",
            "Train Metrics Epoch 17 Loss 0.1169 Accuracy 0.9637\n",
            "Test Metrics Epoch 17 Loss 0.1265 Accuracy 0.9616\n",
            "Train Metrics Epoch 18 Loss 0.1159 Accuracy 0.9637\n",
            "Test Metrics Epoch 18 Loss 0.1433 Accuracy 0.9554\n",
            "Train Metrics Epoch 19 Loss 0.1143 Accuracy 0.9633\n",
            "Test Metrics Epoch 19 Loss 0.1385 Accuracy 0.9575\n"
          ]
        }
      ],
      "source": [
        "# Get the loss function; remember you are outputting the logits\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# Set the optimizer to SGD and let the learning rate be LR\n",
        "# Do not add L2 regularization; add it manually below ...\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "for epoch in range(EPOCHS):\n",
        "    accuracy_hist_train = 0\n",
        "    auroc_hist_train = 0.0\n",
        "    loss_hist_train = 0\n",
        "    # Loop through the x and y pairs of data\n",
        "    for x_batch, y_batch in train_dl:\n",
        "        # Get he the model predictions\n",
        "        y_pred = model(x_batch)\n",
        "\n",
        "        # Get the loss\n",
        "        loss = loss_fn(y_pred, y_batch)\n",
        "\n",
        "        # Add an L1 regularizaton with a weight of L1_WEIGHT to the objective\n",
        "        l1_reg = L1_WEIGHT * torch.norm(model.linear.weight, p=1)\n",
        "\n",
        "        # Add an L2 regularization with a weight of L2_WEIGHT to the objective\n",
        "        l2_reg = L2_WEIGHT * torch.norm(model.linear.weight, p=2)\n",
        "\n",
        "        # Add the regularizers to the objective\n",
        "        loss +=  (l1_reg + l2_reg)\n",
        "\n",
        "        # Get the gradients\n",
        "        loss.backward()\n",
        "\n",
        "        # Add to the loss\n",
        "        # Remember loss: is a mean over the batch size and we need the total sum over the number of samples in the dataset\n",
        "        loss_hist_train += loss.item() * len(y_batch)\n",
        "\n",
        "        # Update the parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        # Zero out the gradient\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Get the number of correct predictions, do this with torchmetrics\n",
        "        is_correct = torchmetrics.Accuracy(task='multiclass', num_classes = 10)(y_pred.argmax(dim=1), y_batch).item() * len(y_batch)\n",
        "\n",
        "        accuracy_hist_train += is_correct\n",
        "    accuracy_hist_train /= len(train_dl.dataset)\n",
        "    loss_hist_train /= len(train_dl.dataset)\n",
        "    print(f'Train Metrics Epoch {epoch} Loss {loss_hist_train:.4f} Accuracy {accuracy_hist_train:.4f}')\n",
        "\n",
        "    accuracy_hist_test = 0\n",
        "    loss_hist_test = 00\n",
        "    # Get the average value of each metric across the test batches\n",
        "    with torch.no_grad():\n",
        "      accuracy_hist_test = 0\n",
        "      auroc_hist_test = 0.0\n",
        "      # Loop through the x and y pairs of data\n",
        "      for x_batch, y_batch in test_dl:\n",
        "          # Get he the model predictions\n",
        "          y_batch_pred = model(x_batch)\n",
        "\n",
        "          # Get the loss\n",
        "          loss = loss_fn(y_batch_pred, y_batch)\n",
        "\n",
        "          # Add an L1 regularizaton with a weight of L1_WEIGHT to the objective\n",
        "          l1_reg = L1_WEIGHT * torch.norm(model.linear.weight, p=1)\n",
        "\n",
        "          # Add an L2 regularization with a weight of L2_WEIGHT to the objective\n",
        "          l2_reg = L2_WEIGHT * torch.norm(model.linear.weight, p=2)\n",
        "\n",
        "          # Add the regularizers to the objective\n",
        "          loss += (l1_reg + l2_reg)\n",
        "\n",
        "          # Add to the loss\n",
        "          # Remember loss: is a mean over the batch size and we need the total sum over the number of samples in the dataset\n",
        "          loss_hist_test += loss.item() * len(y_batch)\n",
        "\n",
        "          # Get the number of correct predictions via torchmetrics\n",
        "          is_correct = torchmetrics.Accuracy(task='multiclass', num_classes = 10)(y_batch_pred.argmax(dim=1), y_batch).item() * len(y_batch)\n",
        "\n",
        "          # Get the accuracy\n",
        "          accuracy_hist_test += is_correct\n",
        "\n",
        "      # Normalize the metrics by the right number\n",
        "      accuracy_hist_test /= len(test_dl.dataset)\n",
        "      loss_hist_test /= len(test_dl.dataset)\n",
        "      print(f'Test Metrics Epoch {epoch} Loss {loss_hist_test:.4f} Accuracy {accuracy_hist_test:.4f}')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
