{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "97ab5757",
      "metadata": {
        "id": "97ab5757"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "c7fb180d",
      "metadata": {
        "id": "c7fb180d"
      },
      "outputs": [],
      "source": [
        "FILL_IN = \"FILL_IN\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7bd215dd",
      "metadata": {
        "id": "7bd215dd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "64c0320b",
      "metadata": {
        "id": "64c0320b"
      },
      "source": [
        "### Get the data and process\n",
        "- This is the Mysterious island found in Project Gutenberg."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "d4e64a98",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4e64a98",
        "outputId": "8f77e735-8243-406c-a7c3-6a82850b2397"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total Length: 1130711\n",
            "Unique Characters: 85\n"
          ]
        }
      ],
      "source": [
        "## Reading and processing text\n",
        "with open('/content/1268-0.txt', 'r', encoding=\"utf8\") as fp:\n",
        "    text=fp.read()\n",
        "\n",
        "# Get the index of 'THE MYSTERIOUS ISLAND' or 'The Mysterious Island'\n",
        "start_indx = text.find('THE MYSTERIOUS ISLAND') if 'THE MYSTERIOUS ISLAND' in text else text.find('The Mysterious Island')\n",
        "# Get the index of 'End of the Project Gutenberg'\n",
        "end_indx = text.find('End of the Project Gutenberg')\n",
        "\n",
        "# Set text to the text between start and end idx.\n",
        "text = text[start_indx:end_indx]\n",
        "# Get the unique set of characters.\n",
        "char_set = set(text)\n",
        "print('Total Length:', len(text))\n",
        "print('Unique Characters:', len(char_set))\n",
        "assert(len(text) == 1130711)\n",
        "assert(len(char_set) == 85)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f650c1d4",
      "metadata": {
        "id": "f650c1d4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "76393bdb",
      "metadata": {
        "id": "76393bdb"
      },
      "source": [
        "### Tokenze and get other helpers\n",
        "- We do this manually since everything is character based."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "3a445114",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3a445114",
        "outputId": "610b5b04-83da-44c0-8ca6-f8a3e9dd6d80"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text encoded shape:  (1130711,)\n",
            "THE MYSTERIOUS       == Encoding ==>  [48 36 33  1 41 53 47 48 33 46 37 43 49 47  1]\n",
            "[37 47 40 29 42 32]  == Reverse  ==>  ISLAND\n"
          ]
        }
      ],
      "source": [
        "# The universe of words.\n",
        "chars_sorted = sorted(char_set)\n",
        "\n",
        "# Effectively, these maps are the tokenizer.\n",
        "# Map each char to a unique int. This is a dict.\n",
        "char2int = {char: i for i, char in enumerate(chars_sorted)}\n",
        "# Do the reverse of the above, this should be a np array.\n",
        "int2char = np.array(chars_sorted)\n",
        "\n",
        "# Tokenize the entire corpus. This should be an np array of np.int32 type.\n",
        "text_encoded = np.array([char2int[char] for char in text], dtype=np.int32)\n",
        "\n",
        "print('Text encoded shape: ', text_encoded.shape)\n",
        "\n",
        "print(text[:15], '     == Encoding ==> ', text_encoded[:15])\n",
        "print(text_encoded[15:21], ' == Reverse  ==> ', ''.join(int2char[text_encoded[15:21]]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d8e0270",
      "metadata": {
        "id": "0d8e0270"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "720cd752",
      "metadata": {
        "id": "720cd752"
      },
      "source": [
        "#### Examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "e2743a57",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2743a57",
        "outputId": "afdf2ada-e48f-453c-f9d8-6e672bc8fcae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text encoded shape:  (1130711,)\n",
            "THE MYSTERIOUS       == Encoding ==>  [48 36 33  1 41 53 47 48 33 46 37 43 49 47  1]\n",
            "[37 47 40 29 42 32]  == Reverse  ==>  ISLAND\n"
          ]
        }
      ],
      "source": [
        "print('Text encoded shape: ', text_encoded.shape)\n",
        "print(text[:15], '     == Encoding ==> ', text_encoded[:15])\n",
        "print(text_encoded[15:21], ' == Reverse  ==> ', ''.join(int2char[text_encoded[15:21]]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "367e733d",
      "metadata": {
        "id": "367e733d"
      },
      "outputs": [],
      "source": [
        "assert(\n",
        "    np.array_equal(\n",
        "    text_encoded[:15],\n",
        "        [48, 36, 33, 1, 41, 53, 47, 48, 33, 46, 37, 43, 49, 47,  1]\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2cdcafe4",
      "metadata": {
        "id": "2cdcafe4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "0c418ca0",
      "metadata": {
        "id": "0c418ca0"
      },
      "source": [
        "### Process the data and get the data loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "f429dc3d",
      "metadata": {
        "id": "f429dc3d"
      },
      "outputs": [],
      "source": [
        "seq_length = 40\n",
        "chunk_size = seq_length + 1\n",
        "\n",
        "# Break up the data into chunks of size 41. This should be a list of lists.\n",
        "# Use text_encoded. This will be used to get (x, y) pairs.\n",
        "text_chunks = [text_encoded[i:i + chunk_size] for i in range(0, len(text_encoded) - chunk_size + 1)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "4qFEz-dPu0uZ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4qFEz-dPu0uZ",
        "outputId": "29d0dba9-3ed5-4725-e1ce-93197286069b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([48, 36, 33,  1, 41, 53, 47, 48, 33, 46, 37, 43, 49, 47,  1, 37, 47,\n",
              "       40, 29, 42, 32,  1, 10, 10, 10,  0,  0,  0,  0,  0, 48, 36, 33,  1,\n",
              "       41, 53, 47, 48, 33, 46, 37], dtype=int32)"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text_chunks[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "e329fffd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e329fffd",
        "outputId": "d8bca275-198a-4961-d6e8-42de315d868b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-12-a1da84902cd5>:15: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:261.)\n",
            "  seq_dataset = TextDataset(torch.tensor(text_chunks))\n"
          ]
        }
      ],
      "source": [
        "class TextDataset(Dataset):\n",
        "    def __init__(self, text_chunks):\n",
        "        self.text_chunks = text_chunks\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text_chunks)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Get the text chunk at index idx.\n",
        "        text_chunk = torch.tensor(self.text_chunks[idx], dtype=torch.long)\n",
        "        # Return (x, y) where x has length 40 and y has length 40.\n",
        "        # y should be x shifted by 1 time.\n",
        "        return text_chunk[:-1], text_chunk[1:]\n",
        "\n",
        "seq_dataset = TextDataset(torch.tensor(text_chunks))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "71328555",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71328555",
        "outputId": "8572cf0a-c18f-45e9-dfc0-e2ec4085991f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([40]) torch.Size([40])\n",
            "Input (x): 'THE MYSTERIOUS ISLAND ***\\n\\n\\n\\n\\nTHE MYSTER'\n",
            "Target (y): 'HE MYSTERIOUS ISLAND ***\\n\\n\\n\\n\\nTHE MYSTERI'\n",
            "\n",
            "torch.Size([40]) torch.Size([40])\n",
            "Input (x): 'HE MYSTERIOUS ISLAND ***\\n\\n\\n\\n\\nTHE MYSTERI'\n",
            "Target (y): 'E MYSTERIOUS ISLAND ***\\n\\n\\n\\n\\nTHE MYSTERIO'\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-12-a1da84902cd5>:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_chunk = torch.tensor(self.text_chunks[idx], dtype=torch.long)\n"
          ]
        }
      ],
      "source": [
        "for i, (seq, target) in enumerate(seq_dataset):\n",
        "    # 40 characters for source and target ...\n",
        "    print(seq.shape, target.shape)\n",
        "    print('Input (x):', repr(''.join(int2char[seq])))\n",
        "    print('Target (y):', repr(''.join(int2char[target])))\n",
        "    print()\n",
        "    if i == 1:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "ebb989c3",
      "metadata": {
        "id": "ebb989c3"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "a881b316",
      "metadata": {
        "id": "a881b316"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "torch.manual_seed(1)\n",
        "seq_dl = DataLoader(seq_dataset, batch_size=batch_size, shuffle=True, drop_last=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f77f7f8",
      "metadata": {
        "id": "0f77f7f8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "45ed0b2f",
      "metadata": {
        "id": "45ed0b2f"
      },
      "source": [
        "### Write the models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "1b4cbf1e",
      "metadata": {
        "id": "1b4cbf1e"
      },
      "outputs": [],
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, rnn_hidden_size):\n",
        "        super().__init__()\n",
        "        # Set to an embedding layer of vocab_size by embed_dim.\n",
        "        self.embedding = nn.Embedding(\n",
        "            vocab_size,\n",
        "            embed_dim\n",
        "        )\n",
        "        self.rnn_hidden_size = rnn_hidden_size\n",
        "        # Set to an LSTM with x having embed_dim and h dimension rnn_hidden_size.\n",
        "        # batch_first should be true.\n",
        "        self.rnn = nn.LSTM(\n",
        "            embed_dim,\n",
        "            rnn_hidden_size,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        # Make a linear layer from rnn_hidden_size to vocab_size.\n",
        "        # This will be used to get the yt for each xt.\n",
        "        self.fc = nn.Linear(rnn_hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, text, hidden=None, cell=None):\n",
        "        # Get the embeddings for text.\n",
        "        out = self.embedding(text)\n",
        "\n",
        "        # Pass out, hidden and cell through the rnn.\n",
        "        # If hidden is None, don't specify it and just use out.\n",
        "        if hidden is not None:\n",
        "            out, (hidden, cell) = self.rnn(out, (hidden, cell))\n",
        "        else:\n",
        "            out, (hidden, cell) = self.rnn(out)\n",
        "\n",
        "        # Pass out through fc.\n",
        "        out = self.fc(out)\n",
        "\n",
        "        return out, (hidden, cell)\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        # hidden, cell are parameters of LSTM. h_t is the hidden state at time t. c_t is the cell state of time t.\n",
        "        # Initialize to zeros of 1 by ??? appropriate dimensions.\n",
        "        hidden = torch.zeros(1, batch_size, self.rnn_hidden_size)\n",
        "        cell = torch.zeros(1, batch_size, self.rnn_hidden_size)\n",
        "        return hidden.to(device), cell.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f16c03dc",
      "metadata": {
        "id": "f16c03dc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "00789dfd",
      "metadata": {
        "id": "00789dfd"
      },
      "source": [
        "### Do this right way - across all data all at once!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "33380607",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33380607",
        "outputId": "93eb954a-89b0-4b81-b153-be5cfebd53e2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "RNN(\n",
              "  (embedding): Embedding(85, 256)\n",
              "  (rnn): LSTM(256, 512, batch_first=True)\n",
              "  (fc): Linear(in_features=512, out_features=85, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vocab_size = len(int2char)\n",
        "embed_dim = 256\n",
        "rnn_hidden_size = 512\n",
        "\n",
        "torch.manual_seed(1)\n",
        "model = RNN(vocab_size, embed_dim, rnn_hidden_size)\n",
        "model = model.to(device)\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "2f47f48a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2f47f48a",
        "outputId": "73a3b8a1-3c36-4810-f460-26e9ded49141"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-12-a1da84902cd5>:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_chunk = torch.tensor(self.text_chunks[idx], dtype=torch.long)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 loss: 2.4586\n",
            "Epoch 100 loss: 1.6735\n",
            "Epoch 200 loss: 1.5128\n",
            "Epoch 300 loss: 1.3801\n",
            "Epoch 400 loss: 1.4316\n",
            "Epoch 500 loss: 1.3273\n",
            "Epoch 600 loss: 1.3108\n",
            "Epoch 700 loss: 1.3075\n",
            "Epoch 800 loss: 1.2872\n",
            "Epoch 900 loss: 1.2563\n"
          ]
        }
      ],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
        "\n",
        "# Set to 10000.\n",
        "num_epochs = 1000\n",
        "\n",
        "torch.manual_seed(1)\n",
        "\n",
        "# epochs here will mean batches.\n",
        "# If the above takes too long, use 1000.\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    hidden, cell = model.init_hidden(batch_size)\n",
        "\n",
        "    # Get the next batch from seq_dl\n",
        "    seq_batch, target_batch = next(iter(seq_dl))\n",
        "\n",
        "    seq_batch = seq_batch.to(device)\n",
        "    target_batch = target_batch.to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    # Pass through the model.\n",
        "    logits, _ = model(seq_batch, hidden, cell)\n",
        "\n",
        "    # Get the loss.\n",
        "    # You'll need to reshape / view things to make this work.\n",
        "    loss += criterion(logits.view(-1, logits.shape[2]), target_batch.view(-1))\n",
        "\n",
        "    # Do back prop.\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    # Get the value in the tensor loss.\n",
        "    loss = loss.item()\n",
        "\n",
        "    if epoch % 100 == 0:\n",
        "        print(f'Epoch {epoch} loss: {loss:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17af6f8e",
      "metadata": {
        "id": "17af6f8e"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "0f398f67",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0f398f67",
        "outputId": "9363f43f-2175-4427-8a2f-5002910f3cb8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Probabilities: tensor([[0.0159, 0.1173, 0.8668]])\n",
            "[[1]\n",
            " [2]\n",
            " [2]\n",
            " [2]\n",
            " [2]\n",
            " [1]\n",
            " [2]\n",
            " [2]\n",
            " [2]\n",
            " [2]]\n"
          ]
        }
      ],
      "source": [
        "from torch.distributions.categorical import Categorical\n",
        "\n",
        "torch.manual_seed(1)\n",
        "\n",
        "logits = torch.tensor([[-1.0, 1.0, 3.0]])\n",
        "\n",
        "# Get the probabilities for these logits.\n",
        "print('Probabilities:', torch.nn.functional.softmax(logits, dim=-1))\n",
        "\n",
        "# Get a Categorical random variable with the above probabilities for each of the classes.\n",
        "m = Categorical(torch.nn.functional.softmax(logits, dim=-1))\n",
        "# Generate 10 things.\n",
        "samples = m.sample((10,))\n",
        "\n",
        "print(samples.numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81ec176d",
      "metadata": {
        "id": "81ec176d"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "0547467d",
      "metadata": {
        "id": "0547467d"
      },
      "source": [
        "### Random decoding.\n",
        "- This compounds problems: once you make a mistake, you can't undo it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "614fb236",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "614fb236",
        "outputId": "f8a79207-1ada-4fe5-e4ca-d0155ee6ca79"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The island.\n",
            "\n",
            "On the windered on the island plants under the wants of the\n",
            "river’s broke damp, so fire,” the water-dissing still the interest on the brig, if they should finary formed his a\n",
            "manufact, coving on this unished, which descended in deep potself interfectives which, I’ve Pencroft of trees, could not appear, he was yeterged\n",
            "violence through it\n",
            "in a hank, which repast, put at low\n",
            "grass, where they were droved three back-and what course Lord Glangarrapulate, and some dark, giving the attach and glass\n"
          ]
        }
      ],
      "source": [
        "def random_sample(\n",
        "    model,\n",
        "    starting_str,\n",
        "    len_generated_text=500,\n",
        "):\n",
        "\n",
        "    # Encode starting string into a tensor using char2str.\n",
        "    encoded_input = torch.tensor([char2int[s] for s in starting_str])\n",
        "\n",
        "    # Reshape to be 1 by ??? - let PyTorch figure this out.\n",
        "    encoded_input = encoded_input.view(1, -1)\n",
        "\n",
        "    # This will be what you generate, but it starts off with something.\n",
        "    generated_str = starting_str\n",
        "\n",
        "    # Put model in eval mode. This matters if we had dropout o batch / layer norms.\n",
        "    model.eval()\n",
        "\n",
        "    hidden, cell = model.init_hidden(1)\n",
        "\n",
        "    hidden = hidden.to(device)\n",
        "\n",
        "    cell = cell.to(device)\n",
        "\n",
        "    # Build up the starting hidden and cell states.\n",
        "    # You can do this all in one go?\n",
        "    for c in range(len(starting_str)-1):\n",
        "        # Feed each letter 1 by 1 and then get the final hidden state.\n",
        "        out, (hidden, cell) = model(encoded_input[:, c].view(1, 1), hidden, cell)\n",
        "        # Pass out through, note we update hidden and cell and use them again\n",
        "        # _, (hidden, cell) = FILL_IN\n",
        "\n",
        "    # Get the last char; note we did not do go to the last char above.\n",
        "    last_char = encoded_input[:, -1]\n",
        "    # Generate chars one at a time, add them to generated_str.\n",
        "    # Do this over and over until you get the desired length.\n",
        "    for i in range(len_generated_text):\n",
        "\n",
        "        # Use hidden and cell from the above.\n",
        "        # Use last_char, which will be updated over and over.\n",
        "        logits, (hidden, cell) = model(last_char.view(1, 1), hidden, cell)\n",
        "\n",
        "        # Get the logits.\n",
        "        logits = logits.view(-1)\n",
        "\n",
        "        # m is a random variable with probabilities based on the softmax of the logits.\n",
        "        m = Categorical(torch.nn.functional.softmax(logits, dim=-1))\n",
        "\n",
        "        # Generate from m 1 char.\n",
        "        last_char = m.sample().view(1)\n",
        "\n",
        "        # Add the geenrated char to generated_str, but pass it through int2str so that\n",
        "        generated_str += int2char[last_char.item()]\n",
        "\n",
        "    return generated_str\n",
        "\n",
        "torch.manual_seed(1)\n",
        "model.to(device)\n",
        "print(random_sample(model, starting_str='The island'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83f58492",
      "metadata": {
        "id": "83f58492"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
