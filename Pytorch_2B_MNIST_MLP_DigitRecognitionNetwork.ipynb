{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRfgHr2GyNTO"
      },
      "source": [
        "For this notebook, please insert where there is `_FILL_` either code or logic to make this work.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5XzVh-J0-fu"
      },
      "source": [
        "# MNIST MLP Digit Recognition Network\n",
        "\n",
        "For this problem, you will code a basic digit recognition network. The data are images which specify the digits 1 to 10 as (1, 28, 28) data - this data is black and white images. Each pixed of the image is an intensity between 0 and 255, and together the (1, 28, 28) pixel image can be visualized as a picture of a digit. The data is given to you as $\\{(x^{(i)}, y^{(i)})\\}_{i=1}^{N}$ where $y$ is the given label and x is the (1, 28, 28) data. This data will be gotten from `torchvision`, a repository of computer vision data and models.\n",
        "\n",
        "Highlevel, the model and notebook goes as follows:\n",
        "*   You first download the data and specify the batch size of B = 16. Each image will need to be turned from a (1, 28, 28) volume into a vector of dimension 784 = 1 * 28 * 28. So each batch will be of size (16, 784).\n",
        "*   Then, you pass the model through two hidden layers, one of dimension (784, 32) and another of dimension (32, 16). After each linear map, you pass the data through a TanH nonlinearity.\n",
        "*   Finally, you pass the data through a (16, 10) linear layer and you return the log softmax of the data.\n",
        "*   What objective do you use? Be careful!\n",
        "*   How do you compute accuracy both manually and with torchmetrics?\n",
        "*   How do you compute AUROC?\n",
        "\n",
        "See the comments below and fill in the analysis where there is `_FILL_` specified. All asserts should pass and accuracy should be higher than 85%. If you use another nonlinearity, like ReLU, you might get higher. Play around with this but submit working code that does better than 85%.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GfKuo0JFxJ7d",
        "outputId": "3ff8cd47-63e2-4110-d865-0de9095d3e00"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-1.2.0-py3-none-any.whl (805 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m805.2/805.2 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.23.5)\n",
            "Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.0.1+cu118)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
            "  Downloading lightning_utilities-0.9.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: packaging>=17.1 in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (23.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.12.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.1->torchmetrics) (3.27.6)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.1->torchmetrics) (17.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.1->torchmetrics) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.1->torchmetrics) (1.3.0)\n",
            "Installing collected packages: lightning-utilities, torchmetrics\n",
            "Successfully installed lightning-utilities-0.9.0 torchmetrics-1.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torchmetrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8UDIb4ldyj2C"
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "from torchvision import transforms\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch.nn as nn\n",
        "import torchmetrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "PdLoOr08AUY5"
      },
      "outputs": [],
      "source": [
        "SEED = 1\n",
        "torch.manual_seed(SEED)\n",
        "_FILL_ = '_FILL_'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jLD0oQmgxxlR",
        "outputId": "935e5adc-9988-473c-bd9c-f145f1d8c5a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 122499119.74it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./MNIST/raw/train-images-idx3-ubyte.gz to ./MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 95911079.83it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./MNIST/raw/train-labels-idx1-ubyte.gz to ./MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "100%|██████████| 1648877/1648877 [00:00<00:00, 31428441.44it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./MNIST/raw/t10k-images-idx3-ubyte.gz to ./MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 21429166.22it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./MNIST/raw/t10k-labels-idx1-ubyte.gz to ./MNIST/raw\n",
            "\n"
          ]
        }
      ],
      "source": [
        "image_path = './'\n",
        "\n",
        "# Use ToTensor to transform the data and scale it by 255 #??\n",
        "# Look up transforms and Compose as well\n",
        "# transforms.ToTensor():\n",
        "# Converts a PIL Image or numpy.ndarray (H x W x C) in the range [0, 255] to\n",
        "# a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0]\n",
        "# if the PIL Image belongs to one of the modes\n",
        "transform = transforms.ToTensor()\n",
        "# transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "mnist_train_dataset = torchvision.datasets.MNIST(\n",
        "    root=image_path,\n",
        "    train=True,\n",
        "    transform=transform,\n",
        "    download=True\n",
        "  )\n",
        "\n",
        "mnist_test_dataset = torchvision.datasets.MNIST(\n",
        "    root=image_path,\n",
        "    train=False,\n",
        "    transform=transform,\n",
        "    download=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "xGLuLaEXyzoD"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 64\n",
        "LR = 0.001\n",
        "EPOCHS = 20\n",
        "# Define the DL for train and test\n",
        "train_dl = DataLoader(mnist_train_dataset, BATCH_SIZE, shuffle=True)\n",
        "test_dl = DataLoader(mnist_test_dataset, BATCH_SIZE, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "PjLznvm8xqaT"
      },
      "outputs": [],
      "source": [
        "class MLPClassifier(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    # Define the layers\n",
        "    self.linear1 = nn.Linear(784, 32)\n",
        "    self.linear2 = nn.Linear(32, 16)\n",
        "    self.linear3 = nn.Linear(16, 10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Flatten x to be of last dimension 784\n",
        "    x = x.view(x.size(0), -1)\n",
        "\n",
        "    # Pass through linear layer 1\n",
        "    x = self.linear1(x)\n",
        "\n",
        "    # Apply tanh\n",
        "    x = nn.functional.tanh(x)\n",
        "\n",
        "    # Pass through linear layer 2\n",
        "    x = self.linear2(x)\n",
        "\n",
        "    # Apply tanh\n",
        "    x = nn.functional.tanh(x)\n",
        "\n",
        "    # Pass through linear layer 3\n",
        "    x = self.linear3(x)\n",
        "\n",
        "    # Return the LogSoftmax of the data\n",
        "    # This will affect the loss we choose below\n",
        "    return nn.functional.log_softmax(x, dim=1)\n",
        "\n",
        "model = MLPClassifier()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zu4Z0aptxqcu",
        "outputId": "7925794f-36ca-4a24-cb20-40131126f5e7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score\n",
            "  warnings.warn(*args, **kwargs)  # noqa: B028\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Metrics Epoch 0 Loss 2.2644 Accuracy 0.2251 AUROC 0.7626\n",
            "Test Metrics Epoch 0 Loss 2.2155 Accuracy 0.3305 AUROC 0.8753\n",
            "Train Metrics Epoch 1 Loss 2.1683 Accuracy 0.4055 AUROC 0.8955\n",
            "Test Metrics Epoch 1 Loss 2.1078 Accuracy 0.5435 AUROC 0.9100\n",
            "Train Metrics Epoch 2 Loss 2.0493 Accuracy 0.5686 AUROC 0.9182\n",
            "Test Metrics Epoch 2 Loss 1.9734 Accuracy 0.6048 AUROC 0.9290\n",
            "Train Metrics Epoch 3 Loss 1.9050 Accuracy 0.6139 AUROC 0.9329\n",
            "Test Metrics Epoch 3 Loss 1.8189 Accuracy 0.6390 AUROC 0.9396\n",
            "Train Metrics Epoch 4 Loss 1.7482 Accuracy 0.6353 AUROC 0.9403\n",
            "Test Metrics Epoch 4 Loss 1.6618 Accuracy 0.6526 AUROC 0.9459\n",
            "Train Metrics Epoch 5 Loss 1.5959 Accuracy 0.6420 AUROC 0.9471\n",
            "Test Metrics Epoch 5 Loss 1.5162 Accuracy 0.6528 AUROC 0.9515\n",
            "Train Metrics Epoch 6 Loss 1.4587 Accuracy 0.6486 AUROC 0.9530\n",
            "Test Metrics Epoch 6 Loss 1.3877 Accuracy 0.6642 AUROC 0.9557\n",
            "Train Metrics Epoch 7 Loss 1.3386 Accuracy 0.6676 AUROC 0.9580\n",
            "Test Metrics Epoch 7 Loss 1.2751 Accuracy 0.6939 AUROC 0.9593\n",
            "Train Metrics Epoch 8 Loss 1.2336 Accuracy 0.6986 AUROC 0.9621\n",
            "Test Metrics Epoch 8 Loss 1.1760 Accuracy 0.7282 AUROC 0.9642\n",
            "Train Metrics Epoch 9 Loss 1.1413 Accuracy 0.7330 AUROC 0.9658\n",
            "Test Metrics Epoch 9 Loss 1.0885 Accuracy 0.7586 AUROC 0.9684\n",
            "Train Metrics Epoch 10 Loss 1.0598 Accuracy 0.7625 AUROC 0.9681\n",
            "Test Metrics Epoch 10 Loss 1.0111 Accuracy 0.7843 AUROC 0.9708\n",
            "Train Metrics Epoch 11 Loss 0.9878 Accuracy 0.7849 AUROC 0.9718\n",
            "Test Metrics Epoch 11 Loss 0.9427 Accuracy 0.8056 AUROC 0.9733\n",
            "Train Metrics Epoch 12 Loss 0.9244 Accuracy 0.8023 AUROC 0.9734\n",
            "Test Metrics Epoch 12 Loss 0.8825 Accuracy 0.8203 AUROC 0.9740\n",
            "Train Metrics Epoch 13 Loss 0.8685 Accuracy 0.8163 AUROC 0.9747\n",
            "Test Metrics Epoch 13 Loss 0.8297 Accuracy 0.8280 AUROC 0.9768\n",
            "Train Metrics Epoch 14 Loss 0.8195 Accuracy 0.8257 AUROC 0.9765\n",
            "Test Metrics Epoch 14 Loss 0.7833 Accuracy 0.8357 AUROC 0.9773\n",
            "Train Metrics Epoch 15 Loss 0.7763 Accuracy 0.8335 AUROC 0.9771\n",
            "Test Metrics Epoch 15 Loss 0.7424 Accuracy 0.8415 AUROC 0.9794\n",
            "Train Metrics Epoch 16 Loss 0.7384 Accuracy 0.8400 AUROC 0.9787\n",
            "Test Metrics Epoch 16 Loss 0.7064 Accuracy 0.8463 AUROC 0.9806\n",
            "Train Metrics Epoch 17 Loss 0.7049 Accuracy 0.8445 AUROC 0.9792\n",
            "Test Metrics Epoch 17 Loss 0.6747 Accuracy 0.8507 AUROC 0.9823\n",
            "Train Metrics Epoch 18 Loss 0.6752 Accuracy 0.8488 AUROC 0.9805\n",
            "Test Metrics Epoch 18 Loss 0.6466 Accuracy 0.8550 AUROC 0.9816\n",
            "Train Metrics Epoch 19 Loss 0.6487 Accuracy 0.8534 AUROC 0.9808\n",
            "Test Metrics Epoch 19 Loss 0.6215 Accuracy 0.8593 AUROC 0.9822\n"
          ]
        }
      ],
      "source": [
        "# Get the loss function; remember you are outputting the LogSoftmax so be careful what loss you pick\n",
        "'''\n",
        "nn.CrossEntropyLoss() combines nn.LogSoftmax() (that is, log(softmax(x))) and nn.NLLLoss() in one single class. \n",
        "Therefore, the output from the network that is passed into \n",
        "nn.CrossEntropyLoss needs to be the raw output of the network (called logits), not the output of the softmax function.\n",
        "'''\n",
        "loss_fn = nn.NLLLoss()\n",
        "\n",
        "# Set the optimizer to SGD and let the learning rate be LR\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "for epoch in range(EPOCHS):\n",
        "    accuracy_hist_train = 0\n",
        "    auroc_hist_train = 0.0\n",
        "    loss_hist_train = 0\n",
        "    # Loop through the x and y pairs of data\n",
        "    for x_batch, y_batch in train_dl:\n",
        "        # Get the model predictions\n",
        "        pred = model(x_batch)\n",
        "\n",
        "        # Get the loss\n",
        "        loss = loss_fn(pred, y_batch)\n",
        "\n",
        "        # Get the gradients\n",
        "        loss.backward()\n",
        "\n",
        "        # Add to the loss\n",
        "        # Remember loss: is a mean over the batch size and we need the total sum over the number of samples in the dataset\n",
        "        loss_hist_train += loss.item() * len(y_batch)\n",
        "\n",
        "        # Update the prameters\n",
        "        optimizer.step()\n",
        "\n",
        "        # Zero out the gradient\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Get the number of correct predictions, do this directly\n",
        "        # This should be a tensor\n",
        "        is_correct_1 = (pred.argmax(dim=1) == y_batch)\n",
        "\n",
        "        # Get the number of correct predictions, do this with torchmetrics\n",
        "        # This should be a Float\n",
        "        is_correct_2 = torchmetrics.Accuracy(task=\"multiclass\", num_classes=10)(pred.argmax(dim=1), y_batch).item() * len(y_batch)\n",
        "\n",
        "        assert(is_correct_1.sum() ==  is_correct_2)\n",
        "\n",
        "        accuracy_hist_train += is_correct_2 # is_correct_2 is a scalar\n",
        "\n",
        "        # Get the AUROC - make sure to multiply by the batch length since this is just the AUC over the batch and you want to take a weighted average later\n",
        "        auroc_hist_train += torchmetrics.AUROC(task=\"multiclass\", num_classes=10)(pred, y_batch).item() * len(y_batch)\n",
        "    accuracy_hist_train /= len(train_dl.dataset)\n",
        "    auroc_hist_train /= len(train_dl.dataset)\n",
        "    loss_hist_train /= len(train_dl.dataset)\n",
        "    print(f'Train Metrics Epoch {epoch} Loss {loss_hist_train:.4f} Accuracy {accuracy_hist_train:.4f} AUROC {auroc_hist_train:.4f}')\n",
        "\n",
        "    accuracy_hist_test = 0\n",
        "    auroc_hist_test = 0.0\n",
        "    loss_hist_test = 00\n",
        "    # Get the average value of each metric across the test batches\n",
        "    # Add a \"with\" clause here so that no gradients are computed; we want to just evaluate the model\n",
        "    with torch.no_grad():\n",
        "      accuracy_hist_test = 0\n",
        "      auroc_hist_test = 0.0\n",
        "      # Loop through the x and y pairs of data\n",
        "      for x_batch, y_batch in test_dl:\n",
        "          # Get he the model predictions\n",
        "          pred = model(x_batch)\n",
        "\n",
        "          # Get the loss\n",
        "          loss = loss_fn(pred, y_batch)\n",
        "\n",
        "          # Add to the loss\n",
        "          # Remember loss: is a mean over the batch size and we need the total sum over the number of samples in the dataset\n",
        "          loss_hist_test += loss.item() * len(y_batch)\n",
        "\n",
        "          # Get the number of correct predictions via torchmetrics\n",
        "          is_correct = torchmetrics.Accuracy(task=\"multiclass\", num_classes=10)(pred.argmax(dim=1), y_batch).item() * len(y_batch)\n",
        "\n",
        "          # Get the accuracy\n",
        "          accuracy_hist_test += torchmetrics.Accuracy(task=\"multiclass\", num_classes=10)(pred.argmax(dim=1), y_batch).item() * len(y_batch)\n",
        "\n",
        "          # Get AUROC\n",
        "          auroc_hist_test += torchmetrics.AUROC(task=\"multiclass\", num_classes=10)(pred, y_batch).item() * len(y_batch)\n",
        "      # Normalize the metrics by the right number\n",
        "      accuracy_hist_test /= len(test_dl.dataset)\n",
        "      auroc_hist_test /= len(test_dl.dataset)\n",
        "      loss_hist_test /= len(test_dl.dataset)\n",
        "      print(f'Test Metrics Epoch {epoch} Loss {loss_hist_test:.4f} Accuracy {accuracy_hist_test:.4f} AUROC {auroc_hist_test:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8MvVnJixqhq",
        "outputId": "befe1486-4122-44ca-f1af-408c68f97cc2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total Final Test accuracy: 0.8555\n",
            "Total Final Test accuracy: 0.8593\n"
          ]
        }
      ],
      "source": [
        "# Get train/test final accuracy directly; make sure you normalize the data by 255.0\n",
        "# Should be around 85%\n",
        "pred = model(mnist_train_dataset.data.float() / 255.0 )\n",
        "is_correct = ((pred.argmax(dim=1)) == mnist_train_dataset.targets).float()\n",
        "print(f'Total Final Test accuracy: {is_correct.mean():.4f}')\n",
        "\n",
        "pred = model(mnist_test_dataset.data.float() / 255.0)\n",
        "is_correct = ((pred.argmax(dim=1)) == mnist_test_dataset.targets).float()\n",
        "print(f'Total Final Test accuracy: {is_correct.mean():.4f}')\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
