{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1172569",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.12:0.14.0 pyspark-shell'\n",
    "\n",
    "import re\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf, lower, explode\n",
    "from pyspark.sql.types import StringType, ArrayType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a42dd0f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/lib/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "com.databricks#spark-xml_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-e3ff2b89-7c45-429e-aecc-3588f482e54d;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.databricks#spark-xml_2.12;0.14.0 in central\n",
      "\tfound commons-io#commons-io;2.8.0 in central\n",
      "\tfound org.glassfish.jaxb#txw2;2.3.4 in central\n",
      "\tfound org.apache.ws.xmlschema#xmlschema-core;2.2.5 in central\n",
      ":: resolution report :: resolve 398ms :: artifacts dl 9ms\n",
      "\t:: modules in use:\n",
      "\tcom.databricks#spark-xml_2.12;0.14.0 from central in [default]\n",
      "\tcommons-io#commons-io;2.8.0 from central in [default]\n",
      "\torg.apache.ws.xmlschema#xmlschema-core;2.2.5 from central in [default]\n",
      "\torg.glassfish.jaxb#txw2;2.3.4 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   4   |   0   |   0   |   0   ||   4   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-e3ff2b89-7c45-429e-aecc-3588f482e54d\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 4 already retrieved (0kB/7ms)\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/05/01 03:06:37 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker\n",
      "23/05/01 03:06:37 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster\n",
      "23/05/01 03:06:37 INFO org.apache.spark.SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "23/05/01 03:06:38 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator\n",
      "23/05/01 03:06:41 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/com.databricks_spark-xml_2.12-0.14.0.jar added multiple times to distributed cache.\n",
      "23/05/01 03:06:41 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/commons-io_commons-io-2.8.0.jar added multiple times to distributed cache.\n",
      "23/05/01 03:06:41 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/org.glassfish.jaxb_txw2-2.3.4.jar added multiple times to distributed cache.\n",
      "23/05/01 03:06:41 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/org.apache.ws.xmlschema_xmlschema-core-2.2.5.jar added multiple times to distributed cache.\n"
     ]
    }
   ],
   "source": [
    "# Create Spark Session from Builder\n",
    "spark = SparkSession.builder.getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7b694b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- ns: long (nullable = true)\n",
      " |-- redirect: struct (nullable = true)\n",
      " |    |-- _VALUE: string (nullable = true)\n",
      " |    |-- _title: string (nullable = true)\n",
      " |-- revision: struct (nullable = true)\n",
      " |    |-- comment: string (nullable = true)\n",
      " |    |-- contributor: struct (nullable = true)\n",
      " |    |    |-- id: long (nullable = true)\n",
      " |    |    |-- ip: string (nullable = true)\n",
      " |    |    |-- username: string (nullable = true)\n",
      " |    |-- format: string (nullable = true)\n",
      " |    |-- id: long (nullable = true)\n",
      " |    |-- minor: string (nullable = true)\n",
      " |    |-- model: string (nullable = true)\n",
      " |    |-- parentid: long (nullable = true)\n",
      " |    |-- sha1: string (nullable = true)\n",
      " |    |-- text: struct (nullable = true)\n",
      " |    |    |-- _VALUE: string (nullable = true)\n",
      " |    |    |-- _bytes: long (nullable = true)\n",
      " |    |    |-- _xml:space: string (nullable = true)\n",
      " |    |-- timestamp: timestamp (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# input file\n",
    "fp = 'hdfs:/wiki-small.xml'\n",
    "# create a DataFrame\n",
    "df = spark.read.format('xml').options(rowTag='page').load(fp)\n",
    "# print schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8985f103",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|               title|                text|\n",
      "+--------------------+--------------------+\n",
      "| accessiblecomputing|#redirect [[compu...|\n",
      "|           anarchism|{{short descripti...|\n",
      "|  afghanistanhistory|#redirect [[histo...|\n",
      "|afghanistangeography|#redirect [[geogr...|\n",
      "|   afghanistanpeople|#redirect [[demog...|\n",
      "|afghanistancommun...|#redirect [[commu...|\n",
      "|afghanistantransp...|#redirect [[trans...|\n",
      "| afghanistanmilitary|#redirect [[afgha...|\n",
      "|afghanistantransn...|#redirect [[forei...|\n",
      "| assistivetechnology|#redirect [[assis...|\n",
      "|        amoeboidtaxa|#redirect [[amoeb...|\n",
      "|              autism|#redirect [[autis...|\n",
      "|      albaniahistory|#redirect [[histo...|\n",
      "|       albaniapeople|#redirect [[demog...|\n",
      "|        aswemaythink|#redirect [[as_we...|\n",
      "|   albaniagovernment|#redirect [[polit...|\n",
      "|      albaniaeconomy|#redirect [[econo...|\n",
      "|              albedo|{{short descripti...|\n",
      "|afroasiaticlanguages|#redirect [[afroa...|\n",
      "|  artificallanguages|#redirect [[const...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df1 = df.withColumn(\"text\", lower(col(\"revision.text._VALUE\"))).withColumn(\"title\", lower(col(\"title\"))) \\\n",
    ".select(\"title\", \"text\")\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6c631af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# an UDF to extract links from \"revision\" columns and return a list of \n",
    "def extract_links(text):\n",
    "    # this regex handles all situations needed except the all whitespace or empty string case\n",
    "    pattern = '(?<=\\[\\[)((?:category:)[^#\\|\\]]*?|[^#\\|\\]:]*?)(?=\\||\\]\\])'\n",
    "    links = re.findall(pattern, str(text), flags=re.I | re.M)\n",
    "    # filter out empty strings and strings with all whitespaces\n",
    "    non_empty = [l for l in links if not l.isspace() and len(l)>0]\n",
    "    return non_empty\n",
    "\n",
    "extract_links_udf = udf(extract_links, ArrayType(StringType()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c482ce5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# REGEX testing\n",
    "# text = \"[[    ]][[]][[File:Hercules Musei Capitolini MC1265 n2.jpg|thumb|right|upright|[[Heracles]] with the apple of [[Hesperides]]]]\"\n",
    "# pattern = '(?<=\\[\\[)((?:category:)[^#\\|\\]]*?|[^#\\|\\]:]*?)(?=\\||\\]\\])'\n",
    "# links = re.findall(pattern, text, flags=re.I | re.M)\n",
    "\n",
    "# print(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be61a4fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:===================================================>       (7 + 1) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|               title|                link|\n",
      "+--------------------+--------------------+\n",
      "|\"hello, world!\" p...|                .deb|\n",
      "|\"hello, world!\" p...|3d computer graphics|\n",
      "|\"hello, world!\" p...|ada (programming ...|\n",
      "|\"hello, world!\" p...|            algol 60|\n",
      "|\"hello, world!\" p...|application progr...|\n",
      "|\"hello, world!\" p...|               ascii|\n",
      "|\"hello, world!\" p...|b (programming la...|\n",
      "|\"hello, world!\" p...|               basic|\n",
      "|\"hello, world!\" p...|                bcpl|\n",
      "|\"hello, world!\" p...|           bell labs|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Explode the links list\n",
    "df2 = df1.withColumn(\"links\", extract_links_udf(col(\"text\"))).withColumn(\"link\", explode(\"links\")).drop(\"links\", \"text\")\n",
    "# Sort two columns in Ascending order, and limit to 10 rows\n",
    "df3 = df2.sort(\"title\", \"link\").limit(10)\n",
    "df3.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f93f927",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------                                        \n",
      "Exception happened during processing of request from ('127.0.0.1', 49892)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/miniconda3/lib/python3.8/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/opt/conda/miniconda3/lib/python3.8/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/opt/conda/miniconda3/lib/python3.8/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/opt/conda/miniconda3/lib/python3.8/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 262, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 235, in poll\n",
      "    if func():\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 239, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/usr/lib/spark/python/pyspark/serializers.py\", line 564, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "df3.write.mode(\"overwrite\").csv(\"gs://4121-programming2/notebooks/jupyter/task2.csv\",header=False,sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8fca3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
