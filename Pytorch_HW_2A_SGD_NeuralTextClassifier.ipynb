{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "5db00378",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5db00378",
        "outputId": "e80eeb21-1028-493c-f96e-e6e41d0eb661"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting portalocker\n",
            "  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: portalocker\n",
            "Successfully installed portalocker-2.8.2\n",
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-1.2.0-py3-none-any.whl (805 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m805.2/805.2 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.23.5)\n",
            "Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.1.0+cu118)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
            "  Downloading lightning_utilities-0.9.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: packaging>=17.1 in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (23.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.12.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.1->torchmetrics) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.1->torchmetrics) (1.3.0)\n",
            "Installing collected packages: lightning-utilities, torchmetrics\n",
            "Successfully installed lightning-utilities-0.9.0 torchmetrics-1.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install portalocker\n",
        "!pip install torchmetrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "OzgMJwpmc0iC",
      "metadata": {
        "id": "OzgMJwpmc0iC"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import logging\n",
        "import time\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import random_split\n",
        "import torchtext\n",
        "from torchtext.data.functional import to_map_style_dataset\n",
        "from torchtext.data.utils import get_tokenizer, ngrams_iterator\n",
        "from torchtext.datasets import DATASETS\n",
        "from torchtext.utils import download_from_url\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch.nn.functional as F\n",
        "import torchmetrics\n",
        "\n",
        "_FILL_ = '_FILL_'\n",
        "SEED = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "kh4pAKXgy0tH",
      "metadata": {
        "id": "kh4pAKXgy0tH"
      },
      "source": [
        "For any of these questions, insert code where there is `_FILL_` so that this notebooks runs correctly."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fHdrXN4d1M_2",
      "metadata": {
        "id": "fHdrXN4d1M_2"
      },
      "source": [
        "Short Question\n",
        "\n",
        "Set up the optimization problem below where we take a random y of data and want theta to converge to this y."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "id": "8OsAbUVezIwD",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8OsAbUVezIwD",
        "outputId": "ccd452b8-0cb1-4e79-ddab-b8c5edbe4c88"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch:0 Loss: 0.37639278173446655\n",
            "Epoch:1 Loss: 0.36148756742477417\n",
            "Epoch:2 Loss: 0.34717273712158203\n",
            "Epoch:3 Loss: 0.3334246575832367\n",
            "Epoch:4 Loss: 0.3202209770679474\n",
            "Epoch:5 Loss: 0.30754023790359497\n",
            "Epoch:6 Loss: 0.2953616678714752\n",
            "Epoch:7 Loss: 0.28366535902023315\n",
            "Epoch:8 Loss: 0.2724321782588959\n",
            "Epoch:9 Loss: 0.2616438865661621\n",
            "Epoch:10 Loss: 0.25128284096717834\n",
            "Epoch:11 Loss: 0.2413320243358612\n",
            "Epoch:12 Loss: 0.23177523910999298\n",
            "Epoch:13 Loss: 0.22259698808193207\n",
            "Epoch:14 Loss: 0.21378211677074432\n",
            "Epoch:15 Loss: 0.20531634986400604\n",
            "Epoch:16 Loss: 0.19718578457832336\n",
            "Epoch:17 Loss: 0.18937723338603973\n",
            "Epoch:18 Loss: 0.18187789618968964\n",
            "Epoch:19 Loss: 0.17467552423477173\n",
            "Epoch:20 Loss: 0.16775837540626526\n",
            "Epoch:21 Loss: 0.16111516952514648\n",
            "Epoch:22 Loss: 0.15473498404026031\n",
            "Epoch:23 Loss: 0.14860747754573822\n",
            "Epoch:24 Loss: 0.14272263646125793\n",
            "Epoch:25 Loss: 0.13707081973552704\n",
            "Epoch:26 Loss: 0.13164283335208893\n",
            "Epoch:27 Loss: 0.12642976641654968\n",
            "Epoch:28 Loss: 0.12142314016819\n",
            "Epoch:29 Loss: 0.11661478877067566\n",
            "Epoch:30 Loss: 0.1119968518614769\n",
            "Epoch:31 Loss: 0.10756178200244904\n",
            "Epoch:32 Loss: 0.10330233722925186\n",
            "Epoch:33 Loss: 0.09921157360076904\n",
            "Epoch:34 Loss: 0.09528277069330215\n",
            "Epoch:35 Loss: 0.09150958806276321\n",
            "Epoch:36 Loss: 0.0878857970237732\n",
            "Epoch:37 Loss: 0.08440551161766052\n",
            "Epoch:38 Loss: 0.08106305450201035\n",
            "Epoch:39 Loss: 0.07785296440124512\n",
            "Epoch:40 Loss: 0.074769988656044\n",
            "Epoch:41 Loss: 0.07180909812450409\n",
            "Epoch:42 Loss: 0.06896546483039856\n",
            "Epoch:43 Loss: 0.06623441725969315\n",
            "Epoch:44 Loss: 0.06361152976751328\n",
            "Epoch:45 Loss: 0.061092521995306015\n",
            "Epoch:46 Loss: 0.058673255145549774\n",
            "Epoch:47 Loss: 0.05634979531168938\n",
            "Epoch:48 Loss: 0.05411833897233009\n",
            "Epoch:49 Loss: 0.051975250244140625\n",
            "Epoch:50 Loss: 0.04991701990365982\n",
            "Epoch:51 Loss: 0.04794031009078026\n",
            "Epoch:52 Loss: 0.04604187235236168\n",
            "Epoch:53 Loss: 0.04421861097216606\n",
            "Epoch:54 Loss: 0.04246756061911583\n",
            "Epoch:55 Loss: 0.04078584164381027\n",
            "Epoch:56 Loss: 0.03917071968317032\n",
            "Epoch:57 Loss: 0.03761955350637436\n",
            "Epoch:58 Loss: 0.036129824817180634\n",
            "Epoch:59 Loss: 0.03469908982515335\n",
            "Epoch:60 Loss: 0.03332500159740448\n",
            "Epoch:61 Loss: 0.03200533241033554\n",
            "Epoch:62 Loss: 0.030737927183508873\n",
            "Epoch:63 Loss: 0.029520709067583084\n",
            "Epoch:64 Loss: 0.028351690620183945\n",
            "Epoch:65 Loss: 0.02722897008061409\n",
            "Epoch:66 Loss: 0.026150695979595184\n",
            "Epoch:67 Loss: 0.025115132331848145\n",
            "Epoch:68 Loss: 0.024120572954416275\n",
            "Epoch:69 Loss: 0.023165393620729446\n",
            "Epoch:70 Loss: 0.022248046472668648\n",
            "Epoch:71 Loss: 0.021367020905017853\n",
            "Epoch:72 Loss: 0.0205208919942379\n",
            "Epoch:73 Loss: 0.019708262756466866\n",
            "Epoch:74 Loss: 0.018927816301584244\n",
            "Epoch:75 Loss: 0.01817827671766281\n",
            "Epoch:76 Loss: 0.017458414658904076\n",
            "Epoch:77 Loss: 0.01676706224679947\n",
            "Epoch:78 Loss: 0.016103092581033707\n",
            "Epoch:79 Loss: 0.015465408563613892\n",
            "Epoch:80 Loss: 0.014852980151772499\n",
            "Epoch:81 Loss: 0.014264802448451519\n",
            "Epoch:82 Loss: 0.013699916191399097\n",
            "Epoch:83 Loss: 0.01315739843994379\n",
            "Epoch:84 Loss: 0.012636365368962288\n",
            "Epoch:85 Loss: 0.012135964818298817\n",
            "Epoch:86 Loss: 0.011655379086732864\n",
            "Epoch:87 Loss: 0.011193826794624329\n",
            "Epoch:88 Loss: 0.010750554502010345\n",
            "Epoch:89 Loss: 0.010324829258024693\n",
            "Epoch:90 Loss: 0.009915966540575027\n",
            "Epoch:91 Loss: 0.009523294866085052\n",
            "Epoch:92 Loss: 0.009146172553300858\n",
            "Epoch:93 Loss: 0.008783982135355473\n",
            "Epoch:94 Loss: 0.008436135947704315\n",
            "Epoch:95 Loss: 0.008102063089609146\n",
            "Epoch:96 Loss: 0.007781222462654114\n",
            "Epoch:97 Loss: 0.0074730850756168365\n",
            "Epoch:98 Loss: 0.0071771517395973206\n",
            "Epoch:99 Loss: 0.006892936769872904\n",
            "tensor([0.1000, 0.2000, 0.3000])\n",
            "tensor([[0.1744, 0.2089, 0.2684]], requires_grad=True)\n"
          ]
        }
      ],
      "source": [
        "from torch._functorch.vmap import lazy_load_decompositions\n",
        "# Short Question\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "# Define y to be a target of dimension (1, 3) without a gradient\n",
        "y = torch.tensor([0.1,0.2,0.3], requires_grad=False)\n",
        "\n",
        "# Define theta to be a random tensor of dimension (1, 3) which requires a gradient; we want theta to converge to y\n",
        "theta = torch.randn(1, 3, requires_grad=True)\n",
        "\n",
        "\n",
        "# Define an SGD optimizer with learning rate 0.01 which acts on theta\n",
        "optimizer = torch.optim.SGD([theta], lr=0.01)\n",
        "\n",
        "# Fil in the code below using the optimizer above to get theta to converge to y\n",
        "for epoch in range(100):\n",
        "  # Zero out the gradients of l with respect to theta\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  # Define a loss manually which is ||theta-x||_{2}^{2}, the L2 loss across all components\n",
        "  loss = torch.norm(theta - y, 2) ** 2\n",
        "\n",
        "  print('Epoch:{} Loss: {}'.format(epoch, loss))\n",
        "\n",
        "  # Get the gradients of l with respect to theta\n",
        "  loss.backward()\n",
        "\n",
        "  # Update theta\n",
        "  optimizer.step()\n",
        "\n",
        "# These should look very similar\n",
        "print(y)\n",
        "print(theta)\n",
        "with torch.no_grad():\n",
        "  # Check the y and theta have converged to almost the same thing\n",
        "  loss = torch.norm(theta - y, 2) ** 2 #??\n",
        "  assert (loss.item() - 0.0)**2 <= 0.001"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "MgA6DK8R3BUU",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "MgA6DK8R3BUU",
        "outputId": "0f76299c-a348-488c-9489-f5071aab0567"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'_FILL_'"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Suppose we forget optimizer.zero_grad()\n",
        "# Given an example of what this does and why we WOULD want to do this\n",
        "# Hint: if you are doing batch gradient descent and call optimizer.zero_grad() every 3 batches, what is the gradient represent?\n",
        "\n",
        "'''\n",
        "answer:\n",
        "optimizer.zero_grad() resets the gradients to 0. \n",
        "we want to calculate gradients only based on data in the current batch and epoch.\n",
        "If we do not reset the gradients to 0, gradients would accumulate.\n",
        "For example, if we are doing batch gradient descent and call optimizer.zero_grad() every 3 batches, \n",
        "if gradient of the 1st batch is 0.01, gradient of the 2nd batch is 0.02, gradient of the 3rd batch is 0.03.\n",
        "Then, gradient we use to update the parameter, in the 1st batch is 0.01, in the 2nd batch is 0.01+0.02=0.03, in the 3rd batch is 0.01+0.02+0.03=0.06.\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5tQSyJ9cIOU",
      "metadata": {
        "id": "f5tQSyJ9cIOU"
      },
      "source": [
        "# Neural Text Classifier - Information\n",
        "\n",
        "For this problem, you will build a basic Neural Text Classifier. The problem will take you through some of the steps needed to be done, including the preprocessing.\n",
        "\n",
        "There is alot of helper code here, but your task is to add in code that has `_FILL_` specified. All assertions should pass.\n",
        "\n",
        "The at a high level, the idea of this model goes as follows.\n",
        "- We are given a training set $\\{(x^{(i)}, y^{(i)})\\}_{i=1}^{N}$ where each $x^{(i)}$ is a sentence and $y^{(i)}$ is a class label.\n",
        "- First, we need to loop over $\\{x^{(i)}\\}_{i=1}^{N}$ and get the Vocabulary, the number of unique words we see.\n",
        "- Once we do this, we will express each word as a one-hot representation. To do this, we will use a mapping from a unique word to an integer. For example, \"the\" might get index 3 and if there are 10 words (in the entire Vocabulary) then \"the\" would have a vector representation $x_{the} = (0,0,1,0,0,0,0,0,0,0)$. There will be many words in this Vocabulary, over 13,000. For this example, each word is mapped to a unique integer.\n",
        "- We will feed batches of data to the model and each batch will be transformed into a tensor with words each word transformed to its integer index in VOCAB below.\n",
        "- For example, we might get [[\"the man walks\"], [\"this is a sentence\"]] -> [[\"the\", \"man\", \"walks\"], [\"this\", \"is\", \"a\", \"sentence\"]] -> [[1, 4, 5], [6, 7, 8, 15]]. It depends on what unique integer each word gets.\n",
        "- Different sentences have different numbers of tokens but all batches need to be the same dimension (this is how PyTorch works), so we need a padding token. So, for example, if the batch size is B = 2 and we given two sentences like [\"a b c\", \"a b c d e\"] then as a tensor this will become [[1, 2, 3, 0, 0], [1, 2, 3, 4, 5]] and notice that we padded the first example so that the tensor is of dimension (2, 5) with M = 5. In some sense, in each batch we need to figure out the maximum number of tokens for an instance and pad each instance to have the same length as this longest instance. To do the above, use the [collate function](https://stackoverflow.com/questions/65279115/how-to-use-collate-fn-with-dataloaders). The idea here is that the Dataloader takes in raw data and the collate function is applied to this data, returning formatting tensors we can use later on in the optimization. You'll fill this in, using the hints.  \n",
        "- After padding, we feed batches of data to the classifier, these are of dimension (B, M). For example, we have a batch size of 2 above and M = 5. This will depend on the batch but here the batch size is B.\n",
        "- Once we feed in (B, M) data to the network, we rewrite this as (B, M, vocab_size) by using a one-hot representation for each word.\n",
        "- Then, we do as it hints in the model's forward method. We first take an average agross all the M elements of each element of the batch to get a (B, vocab_size) tensor that represents each instance. We pass this tensor through linear layer and nonlinear layers as unusual. The model returns logits, without the Softmax applied. This is a multiclass classfication task.\n",
        "\n",
        "Finally, we optimize the network and check it's train and validation set accuracies. We'll use both direct methods and torchmetrics to do this. See the Comments for hints on what you need to fill in."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dcb35e9c",
      "metadata": {
        "id": "dcb35e9c"
      },
      "source": [
        "### Information (if interested in more)\n",
        "- torchtext repo: https://github.com/pytorch/text/tree/main/torchtext\n",
        "- torchtext documentation: https://pytorch.org/text/stable/index.html\n",
        "- collate function: https://stackoverflow.com/questions/65279115/how-to-use-collate-fn-with-dataloaders\n",
        "- embedding layer: https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9140e3c",
      "metadata": {
        "id": "b9140e3c"
      },
      "source": [
        "### Constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "c9ace94f",
      "metadata": {
        "id": "c9ace94f"
      },
      "outputs": [],
      "source": [
        "# This is the dataset we will use\n",
        "DATASET = \"AG_NEWS\"\n",
        "DATA_DIR = \".data\"\n",
        "# We will just use CPU here, but if you have time try \"cuda\"\n",
        "DEVICE = \"cpu\"\n",
        "LR = 8.0\n",
        "BATCH_SIZE = 16\n",
        "NUM_EPOCHS = 5\n",
        "MIN_FREQUENCY = 20\n",
        "# Padding valued used; if we have a tensor data x = [[1,2,3], [4, 5], [1,2,3,4,5]] this needs padding\n",
        "# As a tensor, this is t = [[1, 2, 3, 0, 0], [4, 5, 0, 0, 0], [1, 2, 3, 4, 5]]\n",
        "PADDING_VALUE = 0\n",
        "PADDING_IDX = PADDING_VALUE\n",
        "\n",
        "SEED = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "K3zuWGeqcDsI",
      "metadata": {
        "id": "K3zuWGeqcDsI"
      },
      "source": [
        "# Get the tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "16f471ac",
      "metadata": {
        "id": "16f471ac"
      },
      "outputs": [],
      "source": [
        "# A basic tokenizer by using get_tokenizer; pass \"basic_english\"\n",
        "basic_english_tokenizer = get_tokenizer(\"basic_english\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "d1b61bba",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d1b61bba",
        "outputId": "e8c9d1b9-a3ac-436e-8a96-de8de996deba"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['this', 'is', 'some', 'text', '.', '.', '.']"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "basic_english_tokenizer(\"This is some text ...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "68a50055",
      "metadata": {
        "id": "68a50055"
      },
      "outputs": [],
      "source": [
        "# Save the tokenizer as a contant; this is needed later\n",
        "TOKENIZER = basic_english_tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8620a436",
      "metadata": {
        "id": "8620a436"
      },
      "source": [
        "### Get the data and get the vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "1c84225a",
      "metadata": {
        "id": "1c84225a"
      },
      "outputs": [],
      "source": [
        "# Loop through all the (label, text) data and yield a tokenized version of text\n",
        "def yield_tokens(data_iter):\n",
        "    for _, text in data_iter:\n",
        "        yield TOKENIZER(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "rEChT6jDeLXF",
      "metadata": {
        "id": "rEChT6jDeLXF"
      },
      "outputs": [],
      "source": [
        "train_iter = DATASETS[DATASET](root=DATA_DIR, split=\"train\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "J4q-FQ75eM1R",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J4q-FQ75eM1R",
        "outputId": "b962fa14-b604-4f89-e230-37d1206c44fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3 Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\band of ultra-cynics, are seeing green again.\n"
          ]
        }
      ],
      "source": [
        "# An example of what this data looks like\n",
        "for y, x in train_iter:\n",
        "  print(y, x) # y is 3.\n",
        "  break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "affa3375",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "affa3375",
        "outputId": "7e76df35-440b-44c6-e21e-bfd2e035956f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/datapipes/iter/combining.py:333: UserWarning: Some child DataPipes are not exhausted when __iter__ is called. We are resetting the buffer and each child DataPipe will read from the start again.\n",
            "  warnings.warn(\"Some child DataPipes are not exhausted when __iter__ is called. We are resetting \"\n"
          ]
        }
      ],
      "source": [
        "# Use build_vocab_from_iterator to get the the vocabulary\n",
        "# This is essentially a dictionary going from a word to a unique integer\n",
        "# Make sure to specify the specials\n",
        "VOCAB = build_vocab_from_iterator(\n",
        "    yield_tokens(train_iter), # iterator\n",
        "    min_freq = MIN_FREQUENCY,\n",
        "    specials=('<pad>', '<unk>')\n",
        ")\n",
        "\n",
        "# Set the default index to 1\n",
        "# Otherwise, VOCAB['unknownbigword'] will raise an Exception\n",
        "# I.e. we want '<unk>' to be the unknown word\n",
        "VOCAB.set_default_index(VOCAB['<unk>']) ##\n",
        "# set_default_index: return Value of default index if it is set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "62336be9",
      "metadata": {
        "id": "62336be9"
      },
      "outputs": [],
      "source": [
        "assert VOCAB['<unk>'] == 1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "518918c0",
      "metadata": {
        "id": "518918c0"
      },
      "source": [
        "Examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "de48bde8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "de48bde8",
        "outputId": "54d30f7a-fabb-4fb2-8790-8a2030e2ebad"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1, 437, 0, 1)"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "VOCAB['yoyooyoyoy'], VOCAB['house'], VOCAB['<pad>'], VOCAB['<unk>']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "24247d56",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24247d56",
        "outputId": "7c05ecd9-790c-427f-e7c4-4568f68f2d67"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "13798\n"
          ]
        }
      ],
      "source": [
        "print(len(VOCAB))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "574cce1b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "574cce1b",
        "outputId": "51e27b73-c76a-4cd5-cc63-60f8e2929e0d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[437, 437, 4548, 1]"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "VOCAB(TOKENIZER(\"House house houses ThisisnotaKNownWord\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "id": "1be1e272",
      "metadata": {
        "id": "1be1e272"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "df651a44",
      "metadata": {
        "id": "df651a44"
      },
      "source": [
        "### Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "94741f76",
      "metadata": {
        "id": "94741f76"
      },
      "outputs": [],
      "source": [
        "from torchtext.vocab.vocab_factory import Vocab\n",
        "# Utility to transform text into a list of ints\n",
        "# This shoould go \"a b c\" -> [\"a\", \"b\", \"c\"] -> [1, 2, 3], for example\n",
        "def text_pipeline(x):\n",
        "    # Apply tokenizer to x\n",
        "    tokens = TOKENIZER(x)\n",
        "\n",
        "    # Return the Vocab at those tokens\n",
        "    return VOCAB(tokens) # VOCAB: returns a list of tokens\n",
        "\n",
        "# Return a 0 starting version of x\n",
        "# If x = \"1\" this should return 0\n",
        "# If x = \"3\" this should return 2, Etc.\n",
        "def label_pipeline(x):\n",
        "    return int(x)-1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e903610",
      "metadata": {
        "id": "1e903610"
      },
      "source": [
        "Nice link on collate_fn and DataLoader in PyTorch: https://python.plainenglish.io/understanding-collate-fn-in-pytorch-f9d1742647d3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "95311731",
      "metadata": {
        "id": "95311731"
      },
      "outputs": [],
      "source": [
        "# For a batch of data that might not be a tensor, return the batch in ternsor version\n",
        "# batch is a length B lsit of tuples where each element is (label, text)\n",
        "# label is a raw string like \"1\" here; text is a sentence like \"this is about soccer\"\n",
        "def collate_batch(batch):\n",
        "    label_list, text_list = [], []\n",
        "    for (label, text) in batch:\n",
        "        # Get the label from {1, 2, 3, 4} to {0, 1, 2, 3} and append it to label list\n",
        "        label_list.append(label_pipeline(label))\n",
        "\n",
        "        # Return a list of ints\n",
        "        processed_text = text_pipeline(text)\n",
        "        text_list.append(torch.tensor(processed_text))\n",
        "\n",
        "    # Make label_list into a tensor of dtype=torch.int64\n",
        "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
        "\n",
        "    # Pad the sequence\n",
        "    # For Exmaple: if we had 2 elements and [[1, 2], [1,2,3,4]] in the text_list then we want\n",
        "    # to have [[1, 2, 0, 0], [1, 2, 3, 4]] in text_list and text_list is a tensor\n",
        "    # Look up pad_sequence and make sure you specify batch_first=True and specify the padding_value=0\n",
        "    text_list = pad_sequence(text_list, batch_first=True, padding_value=0)\n",
        "\n",
        "    # Return the data and put it on a GPU or CPU, as needed\n",
        "    return label_list.to(DEVICE), text_list.to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "id": "b1292c44",
      "metadata": {
        "id": "b1292c44"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "b5da047d",
      "metadata": {
        "id": "b5da047d"
      },
      "source": [
        "### Get the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "5d2ae25e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5d2ae25e",
        "outputId": "96fcfcde-db6d-4d2d-8a0b-156d12a9525a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The number of classes is 4 ...\n"
          ]
        }
      ],
      "source": [
        "# Get an iterator for the AG_NEWS dataset and get the train version\n",
        "train_iter = DATASETS[DATASET](root=DATA_DIR, split=\"train\")\n",
        "\n",
        "# Use the above to get the number of class elements\n",
        "num_class = len(set([label for label,text in train_iter]))\n",
        "# What are the classes?\n",
        "print(f\"The number of classes is {num_class} ...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "id": "fc52d408",
      "metadata": {
        "id": "fc52d408"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "eca51b36",
      "metadata": {
        "id": "eca51b36"
      },
      "source": [
        "### Set up the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "8ab8cb7c",
      "metadata": {
        "id": "8ab8cb7c"
      },
      "outputs": [],
      "source": [
        "# A very naive model used to classify text\n",
        "class OneHotTextClassificationModel(nn.Module):\n",
        "    def __init__(self, vocab_size, num_class):\n",
        "        super(OneHotTextClassificationModel, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.num_class = num_class\n",
        "\n",
        "        # Have this layer take in data of dimension vocab_size and return data of dimension 100\n",
        "        # Don't use a bias\n",
        "        self.fc1 = nn.Linear(self.vocab_size, 100, bias=False)\n",
        "\n",
        "        # We will not use this, but see below as we want to mimic this layer using one_hot and fc1\n",
        "        self.e = nn.Embedding(self.vocab_size, 100)\n",
        "\n",
        "        # Have this layer take in 100 and return data of dimension num_class\n",
        "        # Don't use a bias\n",
        "        self.fc2 = nn.Linear(100, self.num_class, bias=False)\n",
        "        self.init_weights()\n",
        "\n",
        "        # See forward below; we do not use this but you can use this if you want to to check\n",
        "        self.use_embedding_layer = False\n",
        "\n",
        "    def init_weights(self):\n",
        "        # Initialize the weights of fc1 to the same exact data as what self.e has\n",
        "        # You need to access the data within these layers\n",
        "        # Initialize the bias to zero\n",
        "        # Hint: look at self.e.weight.data and similarly for fc\n",
        "        # Make sure you have the dimensions line up right\n",
        "        self.fc1.weight.data = self.e.weight.data.clone().t()\n",
        "\n",
        "        # Unitialize fc2 to uniform between -0.5 and 0.5\n",
        "        # Hint: \"uniform_\"\n",
        "        initrange = 0.5\n",
        "        self.fc2.weight.data.uniform_(-initrange, initrange) # uniform sampling\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, K = x.shape\n",
        "        # x is of dimension (B, K), where K is the maximum number of tokens in an element of the batch\n",
        "        # Note: We will make this faster later on by using the nn.Embedding layer\n",
        "\n",
        "        # important\n",
        "        # We will not use nn.Embedding, but the code below, a combination of F.one_hot and fc1, should be the SAME effect as the else clause\n",
        "        if not self.use_embedding_layer:\n",
        "          # Transform x to a tensor where each element is one-hot encoded\n",
        "          x = F.one_hot(x, num_classes=self.vocab_size).float() ##\n",
        "          assert(x.shape == (B, K, self.vocab_size))\n",
        "\n",
        "          # Pass x through fc1 to get the row in fc1 correspondng to the row x is\n",
        "          x = self.fc1(x)\n",
        "          assert(x.shape == (B, K, 100))\n",
        "        else:\n",
        "          # Note: the above two steps should be the same as doing the command below\n",
        "          x = self.e(x)\n",
        "          assert(x.shape == (B, K, 100))\n",
        "\n",
        "        # Take the mean of the embedings for all words in each sentence\n",
        "        x = x.mean(dim=1)\n",
        "        assert(x.shape == (B, 100))\n",
        "\n",
        "        # Apply ReLU to x\n",
        "        x = nn.ReLU()(x)\n",
        "        assert(x.shape == (B, 100))\n",
        "\n",
        "        # Pass through fc2\n",
        "        x = self.fc2(x)\n",
        "        assert(x.shape == (B, self.num_class))\n",
        "\n",
        "        # Return the Logits # Logits: raw (non-normalized) predictions that a classification model generates\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "a43d569e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a43d569e",
        "outputId": "61e05e26-fadb-449c-d9ab-c000ad51bf08"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x783757fa8fb0>"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.manual_seed(SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8994ae19",
      "metadata": {
        "id": "8994ae19"
      },
      "source": [
        "### Set up the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "eaaa82a2",
      "metadata": {
        "id": "eaaa82a2"
      },
      "outputs": [],
      "source": [
        "# Map the data to the right format\n",
        "train_iter, test_iter = DATASETS[DATASET]()\n",
        "train_dataset = to_map_style_dataset(train_iter)\n",
        "test_dataset = to_map_style_dataset(test_iter)\n",
        "\n",
        "# Split data into train and validation\n",
        "num_train = int(len(train_dataset) * 0.95)\n",
        "split_train_, split_valid_ = random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n",
        "\n",
        "# Set up different DataLoaders\n",
        "# Make sure you pass collate_fn as the function you wrote above\n",
        "train_dataloader = DataLoader(split_train_, BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
        "valid_dataloader = DataLoader(split_valid_, BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
        "test_dataloader = DataLoader(test_dataset, BATCH_SIZE, shuffle=True, collate_fn=collate_batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "id": "7a5af374",
      "metadata": {
        "id": "7a5af374"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "72b5bb91",
      "metadata": {
        "id": "72b5bb91"
      },
      "source": [
        "### Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "d58cc1a9",
      "metadata": {
        "id": "d58cc1a9"
      },
      "outputs": [],
      "source": [
        "def train(dataloader, model, optimizer, criterion, epoch):\n",
        "    # Put the model in train mode; this does not matter right now\n",
        "    model.train()\n",
        "    total_acc, total_count = 0, 0\n",
        "    total_loss = 0.0\n",
        "    log_interval = 200\n",
        "\n",
        "    for idx, (label, text) in enumerate(dataloader):\n",
        "        # Zero out the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Get the predictions\n",
        "        predicted_label = model(text)\n",
        "\n",
        "        # Get the loss.\n",
        "        loss = loss_fn(input=predicted_label, target=label)\n",
        "\n",
        "        # The loss is computed by taking a mean, get the sum of the terms on the numerator\n",
        "        with torch.no_grad():\n",
        "          total_loss += loss.item() * label.size(0)\n",
        "\n",
        "        # Do back propagation\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the gradients to have max norm 0.1\n",
        "        # Look up torch.nn.utils.clip_grad_norm\n",
        "        torch.nn.utils.clip_grad_norm(model.parameters(), max_norm=0.1)\n",
        "\n",
        "        # Do an optimization step.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Get the accuracy\n",
        "        # predicted_label is (B, num_class) so take the argmax over the right dimension to get the actual label # over each row\n",
        "        # Make sure you do .item() on whaht you get so that you update the accuracy\n",
        "        total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
        "\n",
        "        # Update the total number of items\n",
        "        total_count += label.size(0)\n",
        "        if idx % log_interval == 0 and idx > 0:\n",
        "            print(\n",
        "                \"| epoch {:3d} | {:5d}/{:5d} batches \"\n",
        "                \"| accuracy {:8.3f} \"\n",
        "                \"| loss {:8.3f}\".format(\n",
        "                    epoch, idx,\n",
        "                    len(dataloader),\n",
        "                    total_acc / total_count,\n",
        "                    total_loss / total_count\n",
        "                    )\n",
        "            )\n",
        "            total_acc, total_count, total_loss = 0, 0, 0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "85722617",
      "metadata": {
        "id": "85722617"
      },
      "outputs": [],
      "source": [
        "def evaluate(dataloader, model):\n",
        "    # Put the model in eval model; this does not matter right now\n",
        "    model.eval()\n",
        "\n",
        "    # Set this to Accuracy from torchmetrics; use multiclass and specify the number of labels\n",
        "    accuracy_fn = torchmetrics.Accuracy(num_classes=model.num_class, task='multiclass').to(DEVICE)\n",
        "    total_acc = 0.0\n",
        "    total_count = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx, (label, text) in enumerate(dataloader):\n",
        "            label, text = label.to(DEVICE), text.to(DEVICE)\n",
        "            # Get the predictions\n",
        "            predicted_label = model(text)\n",
        "            # Get the number of samples we have, the denominator of accuracy\n",
        "            total_count += label.size(0)\n",
        "\n",
        "            # Get the total number of times we have the correct predictions, use accuracy_fn\n",
        "            total_acc += accuracy_fn(predicted_label, label).item() * label.size(0)\n",
        "\n",
        "            # Use accuracy_fn from torchmetrics to check that the total number of correct predictions is the same as if you use argmax on predicted_label\n",
        "            # I.e. I want you to use torchmetrics to compute this AND use the same metod as in train above\n",
        "            # Remember to use .item() on the tensor you get and also rememeber number_or_samples * accuracy = total_times_we_have_equality (the numerator of accuracy)\n",
        "            assert (\n",
        "                accuracy_fn(predicted_label, label).item() * label.size(0) == (predicted_label.argmax(1) == label).sum().item()\n",
        "            )\n",
        "\n",
        "    accuracy = total_acc / total_count\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "W3LjZHTdrWW6",
      "metadata": {
        "id": "W3LjZHTdrWW6"
      },
      "source": [
        "# Train the model\n",
        "\n",
        "You should get an accuracy > 80% for the training set. This might take quite a bit of time to run since we use one-hot. Use nn.Embedding if you want to check this quickly. You should get the SAME answer using either method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "eTEl16pIBkTe",
      "metadata": {
        "id": "eTEl16pIBkTe"
      },
      "outputs": [],
      "source": [
        "# Set up the loss function\n",
        "# Note that this should be a multiclass classification problem and you take in logits\n",
        "loss_fn = nn.CrossEntropyLoss().to(DEVICE)\n",
        "\n",
        "# Instantiate the model\n",
        "# Pass in the number of elements in VOCAB and num_class\n",
        "model = OneHotTextClassificationModel(vocab_size=len(VOCAB), num_class=num_class).to(DEVICE)\n",
        "\n",
        "# Instantiate the SGD optimizer with parameters LR\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=LR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "21ba24f3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21ba24f3",
        "outputId": "b0afc61f-6d54-4ef9-ac81-dd04b081e1fc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-23-a300f7a68c2a>:27: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  torch.nn.utils.clip_grad_norm(model.parameters(), max_norm=0.1)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| epoch   1 |   200/ 7125 batches | accuracy    0.322 | loss    1.424\n",
            "| epoch   1 |   400/ 7125 batches | accuracy    0.422 | loss    1.279\n",
            "| epoch   1 |   600/ 7125 batches | accuracy    0.444 | loss    1.244\n",
            "| epoch   1 |   800/ 7125 batches | accuracy    0.497 | loss    1.179\n",
            "| epoch   1 |  1000/ 7125 batches | accuracy    0.531 | loss    1.127\n",
            "| epoch   1 |  1200/ 7125 batches | accuracy    0.528 | loss    1.129\n",
            "| epoch   1 |  1400/ 7125 batches | accuracy    0.539 | loss    1.115\n",
            "| epoch   1 |  1600/ 7125 batches | accuracy    0.574 | loss    1.027\n",
            "| epoch   1 |  1800/ 7125 batches | accuracy    0.572 | loss    1.035\n",
            "| epoch   1 |  2000/ 7125 batches | accuracy    0.606 | loss    0.961\n",
            "| epoch   1 |  2200/ 7125 batches | accuracy    0.613 | loss    0.962\n",
            "| epoch   1 |  2400/ 7125 batches | accuracy    0.594 | loss    1.012\n",
            "| epoch   1 |  2600/ 7125 batches | accuracy    0.644 | loss    0.916\n",
            "| epoch   1 |  2800/ 7125 batches | accuracy    0.635 | loss    0.918\n",
            "| epoch   1 |  3000/ 7125 batches | accuracy    0.636 | loss    0.903\n",
            "| epoch   1 |  3200/ 7125 batches | accuracy    0.647 | loss    0.910\n",
            "| epoch   1 |  3400/ 7125 batches | accuracy    0.651 | loss    0.886\n",
            "| epoch   1 |  3600/ 7125 batches | accuracy    0.655 | loss    0.861\n",
            "| epoch   1 |  3800/ 7125 batches | accuracy    0.649 | loss    0.890\n",
            "| epoch   1 |  4000/ 7125 batches | accuracy    0.683 | loss    0.806\n",
            "| epoch   1 |  4200/ 7125 batches | accuracy    0.670 | loss    0.865\n",
            "| epoch   1 |  4400/ 7125 batches | accuracy    0.685 | loss    0.828\n",
            "| epoch   1 |  4600/ 7125 batches | accuracy    0.678 | loss    0.828\n",
            "| epoch   1 |  4800/ 7125 batches | accuracy    0.686 | loss    0.805\n",
            "| epoch   1 |  5000/ 7125 batches | accuracy    0.697 | loss    0.806\n",
            "| epoch   1 |  5200/ 7125 batches | accuracy    0.713 | loss    0.775\n",
            "| epoch   1 |  5400/ 7125 batches | accuracy    0.708 | loss    0.751\n",
            "| epoch   1 |  5600/ 7125 batches | accuracy    0.696 | loss    0.808\n",
            "| epoch   1 |  5800/ 7125 batches | accuracy    0.723 | loss    0.720\n",
            "| epoch   1 |  6000/ 7125 batches | accuracy    0.727 | loss    0.729\n",
            "| epoch   1 |  6200/ 7125 batches | accuracy    0.716 | loss    0.739\n",
            "| epoch   1 |  6400/ 7125 batches | accuracy    0.720 | loss    0.755\n",
            "| epoch   1 |  6600/ 7125 batches | accuracy    0.712 | loss    0.746\n",
            "| epoch   1 |  6800/ 7125 batches | accuracy    0.728 | loss    0.713\n",
            "| epoch   1 |  7000/ 7125 batches | accuracy    0.721 | loss    0.738\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   1 | time: 1540.98s | valid accuracy    0.716 \n",
            "-----------------------------------------------------------\n",
            "| epoch   2 |   200/ 7125 batches | accuracy    0.739 | loss    0.704\n",
            "| epoch   2 |   400/ 7125 batches | accuracy    0.733 | loss    0.727\n",
            "| epoch   2 |   600/ 7125 batches | accuracy    0.738 | loss    0.675\n",
            "| epoch   2 |   800/ 7125 batches | accuracy    0.728 | loss    0.720\n",
            "| epoch   2 |  1000/ 7125 batches | accuracy    0.738 | loss    0.688\n",
            "| epoch   2 |  1200/ 7125 batches | accuracy    0.751 | loss    0.676\n",
            "| epoch   2 |  1400/ 7125 batches | accuracy    0.743 | loss    0.680\n",
            "| epoch   2 |  1600/ 7125 batches | accuracy    0.741 | loss    0.680\n",
            "| epoch   2 |  1800/ 7125 batches | accuracy    0.735 | loss    0.709\n",
            "| epoch   2 |  2000/ 7125 batches | accuracy    0.739 | loss    0.688\n",
            "| epoch   2 |  2200/ 7125 batches | accuracy    0.747 | loss    0.687\n",
            "| epoch   2 |  2400/ 7125 batches | accuracy    0.756 | loss    0.656\n",
            "| epoch   2 |  2600/ 7125 batches | accuracy    0.774 | loss    0.610\n",
            "| epoch   2 |  2800/ 7125 batches | accuracy    0.762 | loss    0.633\n",
            "| epoch   2 |  3000/ 7125 batches | accuracy    0.770 | loss    0.627\n",
            "| epoch   2 |  3200/ 7125 batches | accuracy    0.761 | loss    0.651\n",
            "| epoch   2 |  3400/ 7125 batches | accuracy    0.769 | loss    0.632\n",
            "| epoch   2 |  3600/ 7125 batches | accuracy    0.755 | loss    0.660\n",
            "| epoch   2 |  3800/ 7125 batches | accuracy    0.758 | loss    0.654\n",
            "| epoch   2 |  4000/ 7125 batches | accuracy    0.773 | loss    0.596\n",
            "| epoch   2 |  4200/ 7125 batches | accuracy    0.749 | loss    0.675\n",
            "| epoch   2 |  4400/ 7125 batches | accuracy    0.755 | loss    0.655\n",
            "| epoch   2 |  4600/ 7125 batches | accuracy    0.772 | loss    0.604\n",
            "| epoch   2 |  4800/ 7125 batches | accuracy    0.764 | loss    0.637\n",
            "| epoch   2 |  5000/ 7125 batches | accuracy    0.776 | loss    0.629\n",
            "| epoch   2 |  5200/ 7125 batches | accuracy    0.787 | loss    0.587\n",
            "| epoch   2 |  5400/ 7125 batches | accuracy    0.771 | loss    0.624\n",
            "| epoch   2 |  5600/ 7125 batches | accuracy    0.774 | loss    0.612\n",
            "| epoch   2 |  5800/ 7125 batches | accuracy    0.774 | loss    0.606\n",
            "| epoch   2 |  6000/ 7125 batches | accuracy    0.771 | loss    0.622\n",
            "| epoch   2 |  6200/ 7125 batches | accuracy    0.757 | loss    0.629\n",
            "| epoch   2 |  6400/ 7125 batches | accuracy    0.789 | loss    0.588\n",
            "| epoch   2 |  6600/ 7125 batches | accuracy    0.772 | loss    0.626\n",
            "| epoch   2 |  6800/ 7125 batches | accuracy    0.766 | loss    0.627\n",
            "| epoch   2 |  7000/ 7125 batches | accuracy    0.766 | loss    0.624\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   2 | time: 1576.21s | valid accuracy    0.821 \n",
            "-----------------------------------------------------------\n",
            "| epoch   3 |   200/ 7125 batches | accuracy    0.782 | loss    0.600\n",
            "| epoch   3 |   400/ 7125 batches | accuracy    0.782 | loss    0.578\n",
            "| epoch   3 |   600/ 7125 batches | accuracy    0.763 | loss    0.648\n",
            "| epoch   3 |   800/ 7125 batches | accuracy    0.793 | loss    0.596\n",
            "| epoch   3 |  1000/ 7125 batches | accuracy    0.792 | loss    0.564\n",
            "| epoch   3 |  1200/ 7125 batches | accuracy    0.782 | loss    0.585\n",
            "| epoch   3 |  1400/ 7125 batches | accuracy    0.790 | loss    0.584\n",
            "| epoch   3 |  1600/ 7125 batches | accuracy    0.794 | loss    0.566\n",
            "| epoch   3 |  1800/ 7125 batches | accuracy    0.781 | loss    0.597\n",
            "| epoch   3 |  2000/ 7125 batches | accuracy    0.775 | loss    0.606\n",
            "| epoch   3 |  2200/ 7125 batches | accuracy    0.801 | loss    0.548\n",
            "| epoch   3 |  2400/ 7125 batches | accuracy    0.808 | loss    0.528\n",
            "| epoch   3 |  2600/ 7125 batches | accuracy    0.788 | loss    0.570\n",
            "| epoch   3 |  2800/ 7125 batches | accuracy    0.798 | loss    0.551\n",
            "| epoch   3 |  3000/ 7125 batches | accuracy    0.780 | loss    0.586\n",
            "| epoch   3 |  3200/ 7125 batches | accuracy    0.799 | loss    0.569\n",
            "| epoch   3 |  3400/ 7125 batches | accuracy    0.794 | loss    0.574\n",
            "| epoch   3 |  3600/ 7125 batches | accuracy    0.818 | loss    0.513\n",
            "| epoch   3 |  3800/ 7125 batches | accuracy    0.791 | loss    0.553\n",
            "| epoch   3 |  4000/ 7125 batches | accuracy    0.805 | loss    0.535\n",
            "| epoch   3 |  4200/ 7125 batches | accuracy    0.794 | loss    0.559\n",
            "| epoch   3 |  4400/ 7125 batches | accuracy    0.788 | loss    0.561\n",
            "| epoch   3 |  4600/ 7125 batches | accuracy    0.786 | loss    0.577\n",
            "| epoch   3 |  4800/ 7125 batches | accuracy    0.789 | loss    0.567\n",
            "| epoch   3 |  5000/ 7125 batches | accuracy    0.798 | loss    0.557\n",
            "| epoch   3 |  5200/ 7125 batches | accuracy    0.810 | loss    0.538\n",
            "| epoch   3 |  5400/ 7125 batches | accuracy    0.812 | loss    0.513\n",
            "| epoch   3 |  5600/ 7125 batches | accuracy    0.815 | loss    0.540\n",
            "| epoch   3 |  5800/ 7125 batches | accuracy    0.797 | loss    0.559\n",
            "| epoch   3 |  6000/ 7125 batches | accuracy    0.807 | loss    0.525\n",
            "| epoch   3 |  6200/ 7125 batches | accuracy    0.810 | loss    0.538\n",
            "| epoch   3 |  6400/ 7125 batches | accuracy    0.803 | loss    0.553\n",
            "| epoch   3 |  6600/ 7125 batches | accuracy    0.823 | loss    0.503\n",
            "| epoch   3 |  6800/ 7125 batches | accuracy    0.801 | loss    0.555\n",
            "| epoch   3 |  7000/ 7125 batches | accuracy    0.801 | loss    0.548\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   3 | time: 1563.83s | valid accuracy    0.831 \n",
            "-----------------------------------------------------------\n",
            "| epoch   4 |   200/ 7125 batches | accuracy    0.819 | loss    0.526\n",
            "| epoch   4 |   400/ 7125 batches | accuracy    0.801 | loss    0.541\n",
            "| epoch   4 |   600/ 7125 batches | accuracy    0.819 | loss    0.499\n",
            "| epoch   4 |   800/ 7125 batches | accuracy    0.808 | loss    0.543\n",
            "| epoch   4 |  1000/ 7125 batches | accuracy    0.811 | loss    0.533\n",
            "| epoch   4 |  1200/ 7125 batches | accuracy    0.802 | loss    0.569\n",
            "| epoch   4 |  1400/ 7125 batches | accuracy    0.823 | loss    0.526\n",
            "| epoch   4 |  1600/ 7125 batches | accuracy    0.814 | loss    0.521\n",
            "| epoch   4 |  1800/ 7125 batches | accuracy    0.809 | loss    0.561\n",
            "| epoch   4 |  2000/ 7125 batches | accuracy    0.812 | loss    0.517\n",
            "| epoch   4 |  2200/ 7125 batches | accuracy    0.822 | loss    0.512\n",
            "| epoch   4 |  2400/ 7125 batches | accuracy    0.822 | loss    0.507\n",
            "| epoch   4 |  2600/ 7125 batches | accuracy    0.798 | loss    0.550\n",
            "| epoch   4 |  2800/ 7125 batches | accuracy    0.820 | loss    0.504\n",
            "| epoch   4 |  3000/ 7125 batches | accuracy    0.806 | loss    0.551\n",
            "| epoch   4 |  3200/ 7125 batches | accuracy    0.825 | loss    0.491\n",
            "| epoch   4 |  3400/ 7125 batches | accuracy    0.822 | loss    0.533\n",
            "| epoch   4 |  3600/ 7125 batches | accuracy    0.821 | loss    0.502\n",
            "| epoch   4 |  3800/ 7125 batches | accuracy    0.811 | loss    0.523\n",
            "| epoch   4 |  4000/ 7125 batches | accuracy    0.829 | loss    0.479\n",
            "| epoch   4 |  4200/ 7125 batches | accuracy    0.813 | loss    0.532\n",
            "| epoch   4 |  4400/ 7125 batches | accuracy    0.820 | loss    0.518\n",
            "| epoch   4 |  4600/ 7125 batches | accuracy    0.828 | loss    0.477\n",
            "| epoch   4 |  4800/ 7125 batches | accuracy    0.809 | loss    0.539\n",
            "| epoch   4 |  5000/ 7125 batches | accuracy    0.825 | loss    0.507\n",
            "| epoch   4 |  5200/ 7125 batches | accuracy    0.821 | loss    0.488\n",
            "| epoch   4 |  5400/ 7125 batches | accuracy    0.827 | loss    0.488\n",
            "| epoch   4 |  5600/ 7125 batches | accuracy    0.819 | loss    0.508\n",
            "| epoch   4 |  5800/ 7125 batches | accuracy    0.821 | loss    0.505\n",
            "| epoch   4 |  6000/ 7125 batches | accuracy    0.823 | loss    0.500\n",
            "| epoch   4 |  6200/ 7125 batches | accuracy    0.828 | loss    0.484\n",
            "| epoch   4 |  6400/ 7125 batches | accuracy    0.817 | loss    0.500\n",
            "| epoch   4 |  6600/ 7125 batches | accuracy    0.812 | loss    0.527\n",
            "| epoch   4 |  6800/ 7125 batches | accuracy    0.817 | loss    0.508\n",
            "| epoch   4 |  7000/ 7125 batches | accuracy    0.824 | loss    0.464\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   4 | time: 1565.13s | valid accuracy    0.818 \n",
            "-----------------------------------------------------------\n",
            "| epoch   5 |   200/ 7125 batches | accuracy    0.823 | loss    0.477\n",
            "| epoch   5 |   400/ 7125 batches | accuracy    0.830 | loss    0.493\n",
            "| epoch   5 |   600/ 7125 batches | accuracy    0.829 | loss    0.484\n",
            "| epoch   5 |   800/ 7125 batches | accuracy    0.825 | loss    0.475\n",
            "| epoch   5 |  1000/ 7125 batches | accuracy    0.835 | loss    0.461\n",
            "| epoch   5 |  1200/ 7125 batches | accuracy    0.823 | loss    0.497\n",
            "| epoch   5 |  1400/ 7125 batches | accuracy    0.813 | loss    0.521\n",
            "| epoch   5 |  1600/ 7125 batches | accuracy    0.817 | loss    0.472\n",
            "| epoch   5 |  1800/ 7125 batches | accuracy    0.815 | loss    0.509\n",
            "| epoch   5 |  2000/ 7125 batches | accuracy    0.830 | loss    0.478\n",
            "| epoch   5 |  2200/ 7125 batches | accuracy    0.832 | loss    0.468\n",
            "| epoch   5 |  2400/ 7125 batches | accuracy    0.834 | loss    0.475\n",
            "| epoch   5 |  2600/ 7125 batches | accuracy    0.840 | loss    0.470\n",
            "| epoch   5 |  2800/ 7125 batches | accuracy    0.822 | loss    0.510\n",
            "| epoch   5 |  3000/ 7125 batches | accuracy    0.835 | loss    0.479\n",
            "| epoch   5 |  3200/ 7125 batches | accuracy    0.829 | loss    0.470\n",
            "| epoch   5 |  3400/ 7125 batches | accuracy    0.831 | loss    0.475\n",
            "| epoch   5 |  3600/ 7125 batches | accuracy    0.836 | loss    0.481\n",
            "| epoch   5 |  3800/ 7125 batches | accuracy    0.842 | loss    0.445\n",
            "| epoch   5 |  4000/ 7125 batches | accuracy    0.829 | loss    0.489\n",
            "| epoch   5 |  4200/ 7125 batches | accuracy    0.843 | loss    0.444\n",
            "| epoch   5 |  4400/ 7125 batches | accuracy    0.832 | loss    0.496\n",
            "| epoch   5 |  4600/ 7125 batches | accuracy    0.821 | loss    0.504\n",
            "| epoch   5 |  4800/ 7125 batches | accuracy    0.824 | loss    0.486\n",
            "| epoch   5 |  5000/ 7125 batches | accuracy    0.835 | loss    0.460\n",
            "| epoch   5 |  5200/ 7125 batches | accuracy    0.837 | loss    0.458\n",
            "| epoch   5 |  5400/ 7125 batches | accuracy    0.824 | loss    0.479\n",
            "| epoch   5 |  5600/ 7125 batches | accuracy    0.821 | loss    0.486\n",
            "| epoch   5 |  5800/ 7125 batches | accuracy    0.825 | loss    0.491\n",
            "| epoch   5 |  6000/ 7125 batches | accuracy    0.838 | loss    0.457\n",
            "| epoch   5 |  6200/ 7125 batches | accuracy    0.817 | loss    0.500\n",
            "| epoch   5 |  6400/ 7125 batches | accuracy    0.822 | loss    0.494\n",
            "| epoch   5 |  6600/ 7125 batches | accuracy    0.832 | loss    0.456\n",
            "| epoch   5 |  6800/ 7125 batches | accuracy    0.837 | loss    0.473\n",
            "| epoch   5 |  7000/ 7125 batches | accuracy    0.828 | loss    0.490\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   5 | time: 1565.82s | valid accuracy    0.846 \n",
            "-----------------------------------------------------------\n",
            "Checking the results of test dataset.\n",
            "test accuracy    0.836\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(1, NUM_EPOCHS + 1):\n",
        "    epoch_start_time = time.time()\n",
        "    train(train_dataloader, model, optimizer, loss_fn, epoch)\n",
        "    accu_val = evaluate(valid_dataloader, model)\n",
        "    print(\"-\" * 59)\n",
        "    print(\n",
        "        \"| end of epoch {:3d} | time: {:5.2f}s | \"\n",
        "        \"valid accuracy {:8.3f} \".format(\n",
        "            epoch,\n",
        "            time.time() - epoch_start_time,\n",
        "            accu_val\n",
        "            )\n",
        "    )\n",
        "    print(\"-\" * 59)\n",
        "\n",
        "print(\"Checking the results of test dataset.\")\n",
        "accu_test = evaluate(test_dataloader, model)\n",
        "print(\"test accuracy {:8.3f}\".format(accu_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71904d5f",
      "metadata": {
        "id": "71904d5f"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
